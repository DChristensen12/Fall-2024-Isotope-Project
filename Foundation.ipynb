{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Code for Isotope Project Fall 2024 made by Daniel Boupha Christensen"
      ],
      "metadata": {
        "id": "Suzaz6UroQWs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FWLUJ_p05Sx"
      },
      "outputs": [],
      "source": [
        "# import all libraries needed for this project\n",
        "# Run this cell\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "import math\n",
        "import re\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can either mount the CSV files from google drive and get the file path from there or get the file path from your local drive."
      ],
      "metadata": {
        "id": "Z7eBa72QezM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data wrangling, manipulation, and visualization methods.\n",
        "# Run this cell and do not edit it unless it is specified within a function that you can\n",
        "\n",
        "#Global Variables\n",
        "\"\"\"These dictionaries will be have the same keys in the following format of keys and values:\n",
        "{\n",
        "  'Isolation': {\n",
        "    'Isotope': {\n",
        "      'Parent': {\n",
        "        'Daughter': {\n",
        "          'Normalized Collision Energy Value': <value>\n",
        "        }\n",
        "      },\n",
        "      'Daughter': {\n",
        "        'Parent': {\n",
        "          'Normalized Collision Energy Value': <value>\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\"\"\"\n",
        "All_alphas_dict_PD = {} # Gets values from the all_calculations_and_graphs function. It will be updated everytime you use the function\n",
        "All_enrichments_dict_PD= {} # Gets values from the all_calculations_and_graphs function. It will be updated everytime you use the function\n",
        "All_Errors_dict_PD = {} # Gets values from the all_calculations_and_graphs function. It will be updated everytime you use the function\n",
        "All_Parent_Areas_dict_PD = {} # Gets values from the all_calculations_and_graphs function. It will be updated everytime you use the function\n",
        "All_Daughter_Areas_dict_PD = {} #Gets values from the all_calculations_and_graphs function. It will be updated everytime you use the function.\n",
        "Daughter_NCE_Values_PD = [] # Gets values from the all_calculations_and_graphs function. It will be updated everytime you use the function\n",
        "Parent_NCE_Values_PD = [] # Gets values from the all_calculations_and_graphs function. It will be updated everytime you use the function\n",
        "\n",
        "\"\"\"All These dictionaries will have the same keys which are the dates the data was taken\"\"\"\n",
        "All_alphas = {} # Gets values in the Error class in the calculate_δα_indv function. It will be updated everytime you use the function\n",
        "All_enrichments = {} # Gets values in the Error class in the calculate_δα_indv function. It will be updated everytime you use the function\n",
        "All_Errors = {} # Gets values in the Error class in the calculate_δα_indv function. It will be updated everytime you use the function\n",
        "All_Parent_Areas = {} # Gets values in the Error class in the calculate_δα_indv function. It will be updated everytime you use the function\n",
        "All_Daughter_Areas = {} # Gets values in the Error class in the calculate_δα_indv function. It will be updated everytime you use the function\n",
        "All_Isotopes = () # Gets values in the Error class in the calculate_δα_indv function.This occurs once as all the files should contain the same isotopes.\n",
        "Parent_masses = () # Gets values in the Error class in the calculate_δα_indv function. This only occurs once as all the files should contain the same parent masses.\n",
        "Daughter_masses = () # Gets values in the Error class in the calculate_δα_indv function. This only occurs once as all the files should contain the same daughter masses.\n",
        "Atom_Names = \" \" # Gets values in the Error class in the calculate_δα_indv function. It will be updated once, since this code doesn't function for graphing different isotopes of different molecules such as with Dy and Nd molecules\n",
        "\n",
        "#Global Functions\n",
        "def clear_GV_PD():\n",
        "  \"\"\"Clears all global variables except the one's that require the use and clearing of the global varibales in clearGV function. They are labeled _PD after the variable name since the distinction between\n",
        "  Parent NCE and Daughter needs to be made.This is for the graphs data analysis that wants to compare individual aspects of the data from multiple isolations and requires the analysis of all\n",
        "  data to be done first, so you use this function in conjunction with clearGV if you want to completely start over.\"\"\"\n",
        "  global All_alphas_dict_PD\n",
        "  global All_enrichments_dict_PD\n",
        "  global All_Errors_dict_PD\n",
        "  global All_Parent_Areas_dict_PD\n",
        "  global All_Daughter_Areas_dict_PD\n",
        "  global Daughter_NCE_Values_PD\n",
        "  global Parent_NCE_Values_PD\n",
        "\n",
        "  All_alphas_dict_PD = {}\n",
        "  All_enrichments_dict_PD= {}\n",
        "  All_Errors_dict_PD = {}\n",
        "  All_Parent_Areas_dict_PD = {}\n",
        "  All_Daughter_Areas_dict_PD = {}\n",
        "  Daughter_NCE_Values_PD = []\n",
        "  Parent_NCE_Values_PD = []\n",
        "\n",
        "def clearGV():\n",
        "  \"\"\"Clears global variables not relating to the global variables in .\"\"\"\n",
        "  global All_alphas\n",
        "  global All_enrichments\n",
        "  global All_Errors\n",
        "  global All_Parent_Areas\n",
        "  global All_Daughter_Areas\n",
        "  global All_Isotopes\n",
        "  global Parent_masses\n",
        "  global Daughter_masses\n",
        "  global Atom_Names\n",
        "  All_alphas = {}\n",
        "  All_enrichments = {}\n",
        "  All_Errors = {}\n",
        "  All_Parent_Areas = {}\n",
        "  All_Daughter_Areas = {}\n",
        "  All_Isotopes = ()\n",
        "  Parent_masses = ()\n",
        "  Daughter_masses = ()\n",
        "  Atom_Names = \" \"\n",
        "\n",
        "def Get_chemical_formula_in_Latex(chemical_formula):\n",
        "    \"\"\"\n",
        "    Takes a chemical formula as input and returns the LaTeX formatted string.\n",
        "    Handles subscripts, superscripts, and charges.\n",
        "    \"\"\"\n",
        "    # Step 1: Handle groups with subscripts (e.g., NO3 -> NO_3, SO4 -> SO_4)\n",
        "    formula = re.sub(r'([A-Za-z]+)(\\d+)', r'\\1_\\2', chemical_formula)  # For elements with subscripts like NaCl2\n",
        "    formula = re.sub(r'(NO3|SO4|CO3|PO4|Cl|Br|I|NH4)(\\d*)', r'\\1_\\2', formula)  # Specific group formatting\n",
        "\n",
        "    # Step 2: Handle superscript for charges (e.g., - -> ^{-})\n",
        "    formula = re.sub(r'\\^-(?=\\])', r'^{-}', formula)  # For negative charge\n",
        "    formula = re.sub(r'\\^(\\d+)', r'^{\\1}', formula)  # For positive charge, e.g., Na^+ to Na^{+}\n",
        "\n",
        "    # Step 3: Wrap the formula with \\mathrm{} for proper chemical notation\n",
        "    latex_formula = r'$\\mathrm{' + formula + r'}$'\n",
        "\n",
        "    return latex_formula\n",
        "\n",
        "def format_chemical_formula(chemical_formula):\n",
        "    \"\"\"\n",
        "    Takes a LaTeX-style chemical formula with _ and ^ for subscripts and superscripts\n",
        "    and returns the properly formatted formula as a string.\n",
        "\n",
        "    Input example:\n",
        "    - 'H_{2}O' for H₂O\n",
        "    - 'NO_{3}^{-}' for NO₃⁻\n",
        "    - 'Nd(NO_{3})_{4}^{-}' for [Nd(NO₃)₄]⁻\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the subscript and superscript maps inside the function\n",
        "    subscript_map = str.maketrans(\"0123456789\", \"₀₁₂₃₄₅₆₇₈₉\")\n",
        "    superscript_map = str.maketrans(\"0123456789\", \"⁰¹²³⁴⁵⁶⁷⁸⁹\")\n",
        "\n",
        "    # Step 1: Convert _{number} to subscripts for individual elements or groups\n",
        "    formula = re.sub(r'_{(\\d+)}', lambda m: m.group(1).translate(subscript_map), chemical_formula)\n",
        "\n",
        "    # Step 2: Convert ^{number} or ^{-} to superscripts for charges or other exponents\n",
        "    formula = re.sub(r'\\^{(-?\\d+)}', lambda m: m.group(1).translate(superscript_map), formula)\n",
        "    formula = re.sub(r'\\^{(-)}', '⁻', formula)  # Handle the negative sign as a superscript\n",
        "\n",
        "    # Step 3: Remove curly braces from around groups (i.e., NO_{3} -> NO₃)\n",
        "    formula = re.sub(r'\\{|\\}', '', formula)\n",
        "\n",
        "    # Step 4: Ensure the minus sign is in superscript form where needed\n",
        "    formula = formula.replace('^-', '⁻')  # Handle the special case for superscript minus\n",
        "\n",
        "    return formula\n",
        "\n",
        "    \"\"\"# Test cases with LaTeX-style input formulas\n",
        "      input_formula_1 = 'H_{2}O'\n",
        "      formatted_formula_1 = format_chemical_formula(input_formula_1)\n",
        "      print(formatted_formula_1)  # Expected Output: H₂O\n",
        "\n",
        "      input_formula_2 = 'NO_{3}^{-}'\n",
        "      formatted_formula_2 = format_chemical_formula(input_formula_2)\n",
        "      print(formatted_formula_2)  # Expected Output: NO₃⁻\n",
        "\n",
        "      input_formula_3 = 'Nd(NO_{3})_{4}^{-}'\n",
        "      formatted_formula_3 = format_chemical_formula(input_formula_3)\n",
        "      print(formatted_formula_3)  # Expected Output: Nd(NO₃)₄⁻\n",
        "\n",
        "      input_formula_4 = '[Nd(NO_{3})_{4}]^{-}'\n",
        "      formatted_formula_4 = format_chemical_formula(input_formula_4)\n",
        "      print(formatted_formula_4)  # Expected Output: [Nd(NO₃)₄]⁻\"\"\"\n",
        "\n",
        "def file_number(file_name):\n",
        "   num_part = file_name.split('_')[1].replace('.csv', '') if '_' in file_name else ''\n",
        "   return num_part\n",
        "\n",
        "def import_google_spreadsheet_file(URL):\n",
        "    \"\"\"This function takes the string of a URl of a google sheet and stores it into a pandas dataframe, then returns it.\n",
        "    If you have any files that are in a google spreadsheet, this function allows you to import it in, however you must Change the sharing\n",
        "    settings to ‘Anyone with the link’ can view, which will make your Google Sheet publicly accessible in a read-only format.\n",
        "\n",
        "    This function is already implemented into the classes below that'd use this funtion.\"\"\"\n",
        "\n",
        "    # Regular expression to match and capture the necessary part of the URL\n",
        "    general_url_pattern = r'https://docs\\.google\\.com/spreadsheets/d/([a-zA-Z0-9-_]+)(/edit#gid=(\\d+)|/edit.*)?'\n",
        "\n",
        "    # Replace function to construct the new URL for CSV export\n",
        "    get_spreadsheet_ID_and_sheet_ID = lambda m: f'https://docs.google.com/spreadsheets/d/{m.group(1)}/export?' + \\\n",
        "                                               (f'gid={m.group(3)}&' if m.group(3) else '') + 'format=csv'\n",
        "\n",
        "    # Replace using regex to construct the export URL\n",
        "    New_URL = re.sub(general_url_pattern, get_spreadsheet_ID_and_sheet_ID, URL)\n",
        "\n",
        "    # Read the CSV from the generated URL, handling bad lines and disabling low memory mode\n",
        "    try:\n",
        "        df = pd.read_csv(New_URL, low_memory=False, on_bad_lines='skip')  # Disable low memory mode and skip bad lines\n",
        "        # Optionally, drop rows that are completely empty\n",
        "        df_cleaned = df.dropna(how='all')\n",
        "    except pd.errors.ParserError as e:\n",
        "        print(\"Error parsing:\", e)\n",
        "        return pd.DataFrame()  # Return an empty DataFrame on error\n",
        "\n",
        "    return df_cleaned\n",
        "\n",
        "\n",
        "def αVIsol(Isotope, constant_NCE='P', NCE_Value_Key_Parent=None, NCE_Value_Key_Daughter=None, save_pdf=False):\n",
        "    \"\"\"\n",
        "    Plot Alpha vs. Isolation for a given Isotope and constant NCE (Parent or Daughter).\n",
        "\n",
        "    Parameters:\n",
        "        Isotope (str): The isotope key to focus on.\n",
        "        constant_NCE (str): 'P' for constant Parent, 'D' for constant Daughter.\n",
        "        NCE_Value_Key_Parent (str or float): The NCE value for the Parent.\n",
        "        NCE_Value_Key_Daughter (str or float): The NCE value for the Daughter.\n",
        "        save_pdf (bool): Whether to save the plot as a PDF.\n",
        "    \"\"\"\n",
        "    # Declare global dictionaries\n",
        "    global All_alphas_dict_PD\n",
        "    global All_Errors_dict_PD\n",
        "\n",
        "    x_vals = []  # Isolation keys (top-level keys in the dictionary)\n",
        "    y_vals = []  # Alpha values\n",
        "    y_errs = []  # Error bars\n",
        "\n",
        "    # Iterate over the isolation keys\n",
        "    for isolation, isotopes in All_alphas_dict_PD.items():\n",
        "        if Isotope not in isotopes:\n",
        "          raise ValueError(f\"Isotope '{Isotope}' not found in the specified isolation '{isolation}'.\")\n",
        "          #continue  # Skip if the specified Isotope is not found. The isotope value SHOULD always be in it.\n",
        "\n",
        "        # Process alphas and errors for the specified isotope\n",
        "        isotope_alphas = All_alphas_dict_PD[isolation][Isotope]\n",
        "        isotope_errors = All_Errors_dict_PD[isolation][Isotope]\n",
        "\n",
        "        if constant_NCE == 'P':\n",
        "            # Parent is constant; gather alphas and errors for Daughters\n",
        "            for parent, daughters in isotope_alphas.items():\n",
        "                for daughter, alpha_value in daughters.items():\n",
        "                    x_vals.append(isolation)\n",
        "                    y_vals.append(alpha_value)\n",
        "                    y_errs.append(isotope_errors[parent][daughter])\n",
        "        elif constant_NCE == 'D':\n",
        "            # Daughter is constant; gather alphas and errors for Parents\n",
        "            for daughter, parents in isotope_alphas.items():\n",
        "                for parent, alpha_value in parents.items():\n",
        "                    x_vals.append(isolation)\n",
        "                    y_vals.append(alpha_value)\n",
        "                    y_errs.append(isotope_errors[daughter][parent])\n",
        "        else:\n",
        "            raise ValueError(\"Invalid constant_NCE value. Must be 'P' or 'D'.\")\n",
        "\n",
        "    # Determine legend label\n",
        "    constant_label = f\"{constant_NCE}_NCE\"  # P_NCE or D_NCE\n",
        "    legend_label = f\"N = {Isotope}, {constant_label} = {NCE_Value_Key_Parent if constant_NCE == 'P' else NCE_Value_Key_Daughter}\"\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.errorbar(\n",
        "        x_vals, y_vals, yerr=y_errs, fmt='o', capsize=5, label=legend_label\n",
        "    )\n",
        "    plt.xlabel(\"Isolation\")\n",
        "    plt.ylabel(\"Alpha Value\")\n",
        "    plt.title(\"Alpha vs. Isolation\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    custom_labels = []\n",
        "    for isotope in All_Isotopes:\n",
        "      isotope_label = f\"$\\\\mathrm{{^{{{isotope}}}{Atom_Names}}}$\"\n",
        "      custom_labels.append(isotope_label)  # Custom labels\n",
        "    x_axis = All_Isotopes\n",
        "    plt.xticks(x_axis, custom_labels)\n",
        "\n",
        "    # Save or display the plot\n",
        "    if not save_pdf:\n",
        "      plt.show()\n",
        "\n",
        "def αVNCE(Isotope, x_axis_NCE, constant_NCE = None, save_pdf = False):\n",
        "    \"\"\"Generates an alpha vs normalized collision energy graph where the x_axis NCE is something that is changed and the constant_NCE is a NCE value the data shares.\n",
        "    If constant_NCE value is equal to None, it will assume alpha is at a constant NCE value and graph the corresponding alpha vs NCE plot. If it is not none, it assumes the\n",
        "    plotted alphas are not under the same NCE values.\"\"\"\n",
        "    return 3\n",
        "\n",
        "def Combined_Data_Line_Plot_EVI(Normalization_Number = 1, Normalization_Number_is_total_areas = True, save_pdf = False):\n",
        "    \"\"\"Makes an Enrichment vs Isotope graph of all combined data after combined the same data stored in the global variables.\n",
        "    It does need a Normalization number which is an integer of the value you want to normalize the data by, put 1 if none. If you want to normalize by total areas,\n",
        "    just put the integer 1 into the function, otherwise put Normalization_Number_is_total_areas = False. Put save_pdf = True if you are in the process of adding this chart to a pdf\"\"\"\n",
        "\n",
        "    # Sums values across all the tuples resulting in one tuple of all the summed areas.\n",
        "    summed_tuple_Parent = tuple(sum(values) for values in zip(*All_Parent_Areas.values()))\n",
        "    summed_tuple_Daughter = tuple(sum(values) for values in zip(*All_Daughter_Areas.values()))\n",
        "\n",
        "    # Turns the tuples into an array to get the ratios which normalizes it\n",
        "    Parent_Ratios = np.array(summed_tuple_Parent) / np.array(summed_tuple_Parent).sum() if Normalization_Number_is_total_areas else np.array(summed_tuple_Parent) / Normalization_Number\n",
        "    Daughter_Ratios = np.array(summed_tuple_Daughter) / np.array(summed_tuple_Daughter).sum() if Normalization_Number_is_total_areas else np.array(summed_tuple_Daughter) / Normalization_Number\n",
        "\n",
        "    # Calculating the errors and enrichments\n",
        "    Enrichments_of_Combined_Data = tuple((Daughter_Ratios/Parent_Ratios) - 1)\n",
        "    Errors_of_Combined_Data = tuple((Daughter_Ratios/Parent_Ratios) * np.sqrt(\n",
        "        (np.sqrt(np.array(summed_tuple_Daughter)) / np.array(summed_tuple_Daughter))**2 +\n",
        "        (np.sqrt(np.array(summed_tuple_Parent)) / np.array(summed_tuple_Parent))**2))\n",
        "\n",
        "    # Making the graph\n",
        "    plt.errorbar(All_Isotopes, Enrichments_of_Combined_Data, yerr = Errors_of_Combined_Data, capsize=3, marker='o', linestyle='-', color = (0.6, 0.8, 1.0), ecolor = (0.5,0.0,0.5), markerfacecolor = (1.0,0.5,0.4), markeredgecolor = (0.9,0.7,0.3))\n",
        "    plt.xlabel('Isotope')\n",
        "    plt.ylabel('ε')\n",
        "    plt.title(\"ε Trends vs. Isotopic Composition for Combined Data\")\n",
        "    plt.grid(True)\n",
        "    custom_labels = []\n",
        "    for isotope in All_Isotopes:\n",
        "      isotope_label = f\"$\\\\mathrm{{^{{{isotope}}}{Atom_Names}}}$\"\n",
        "      custom_labels.append(isotope_label)  # Custom labels\n",
        "    x_axis = All_Isotopes\n",
        "    plt.xticks(x_axis, custom_labels)\n",
        "    if not save_pdf:\n",
        "      plt.show()\n",
        "\n",
        "def Combined_Data_Line_Plot_AVI(Normalization_Number = 1, Normalization_Number_is_total_areas = True, save_pdf = False):\n",
        "    \"\"\"Makes an Alpha vs Isotope graph after combined the data stored in the global variables.\n",
        "    It does need a Normalization number which is an integer of the value you want to normalize the data by, put 1 if none. If you want to normalize by total areas,\n",
        "    just put the integer 1 into the function, otherwise put Normalization_Number_is_total_areas = False. Put save_pdf = True if you are in the process of adding this chart to a pdf\"\"\"\n",
        "\n",
        "    # Sums values across all the tuples resulting in one tuple of all the summed areas.\n",
        "    summed_tuple_Parent = tuple(sum(values) for values in zip(*All_Parent_Areas.values()))\n",
        "    summed_tuple_Daughter = tuple(sum(values) for values in zip(*All_Daughter_Areas.values()))\n",
        "\n",
        "    # Turns the tuples into an array to get the ratios which normalizes it\n",
        "    Parent_Ratios = np.array(summed_tuple_Parent) / np.array(summed_tuple_Parent).sum() if Normalization_Number_is_total_areas else np.array(summed_tuple_Parent) / Normalization_Number\n",
        "    Daughter_Ratios = np.array(summed_tuple_Daughter) / np.array(summed_tuple_Daughter).sum() if Normalization_Number_is_total_areas else np.array(summed_tuple_Daughter) / Normalization_Number\n",
        "\n",
        "    # Calculating the errors and alphas of the newly combined data\n",
        "    Alphas_of_Combined_Data = tuple(Daughter_Ratios / Parent_Ratios)\n",
        "    Errors_of_Combined_Data = tuple((Daughter_Ratios/Parent_Ratios) * np.sqrt(\n",
        "        (np.sqrt(np.array(summed_tuple_Daughter)) / np.array(summed_tuple_Daughter))**2 +\n",
        "        (np.sqrt(np.array(summed_tuple_Parent)) / np.array(summed_tuple_Parent))**2))\n",
        "\n",
        "    # Color themes for the Graph! Put \"\"\" \"\"\" around it if you want the regular default colors\n",
        "    Line_Colors = ('#6dbb22', \"#87CEEB\") #\n",
        "    Dot_Colors = ('#8c5c2f', \"#8B4513\") #\n",
        "    Error_Bar_Colors = ('#4a7023', \"#228B22\")\n",
        "    Color_Themes_Picker = np.array([0,1])\n",
        "    Random_Theme = np.random.choice(Color_Themes_Picker)\n",
        "\n",
        "    # Making the graph\n",
        "    plt.errorbar(All_Isotopes, Alphas_of_Combined_Data, yerr = Errors_of_Combined_Data, capsize=3, marker='o', linestyle='-', color = Line_Colors[Random_Theme], ecolor = Error_Bar_Colors[Random_Theme], markerfacecolor = Dot_Colors[Random_Theme])\n",
        "    plt.xlabel('Isotope')\n",
        "    plt.ylabel('α')\n",
        "    plt.title(\"Combined Data α Trends as a Function of Isotopic Composition\")\n",
        "    plt.grid(True)\n",
        "    custom_labels = []\n",
        "    for isotope in All_Isotopes:\n",
        "      isotope_label = f\"$\\\\mathrm{{^{{{isotope}}}{Atom_Names}}}$\"\n",
        "      custom_labels.append(isotope_label)  # Custom labels\n",
        "    x_axis = All_Isotopes\n",
        "    plt.xticks(x_axis, custom_labels)\n",
        "    if not save_pdf:\n",
        "      plt.show()\n",
        "\n",
        "def Add_Combined_Data_Table_To_PDF(Normalization_Number = 1, Normalization_Number_is_total_areas = True, save_Table = False, PDF_MAKER = None, Pivot = False, Number_of_Decimals = None, additional_Information = None):\n",
        "    \"\"\"Generates a table with columns I defined for combined data for final graphs after using all_calculations_and_graphs multiple times or calculate_δα_indv multiple times.\n",
        "     The table will have columns Total Areas Parent, Total Areas Daughter, α ε, δα, and preference. The rows will correspond to the Isotopes. Make an instance/put an instance of\n",
        "     MakePDF in the PDF_Maker argument to put where it'll store the table. Put pivot as true if the data in the lists will be put in the table vertically as a column.\"\"\"\n",
        "    custom_labels = []\n",
        "    for isotope in All_Isotopes:\n",
        "      isotope_label = f\"$\\\\mathrm{{^{{{isotope}}}{Atom_Names}}}$\"\n",
        "      custom_labels.append(isotope_label)\n",
        "    # Sums values across all the tuples resulting in one tuple of all the summed areas.\n",
        "    summed_tuple_Parent = tuple(sum(values) for values in zip(*All_Parent_Areas.values()))\n",
        "    summed_tuple_Daughter = tuple(sum(values) for values in zip(*All_Daughter_Areas.values()))\n",
        "\n",
        "    Combined_Error_Parent = np.sqrt(np.array(summed_tuple_Parent))\n",
        "    Combined_Error_Daughter = np.sqrt(np.array(summed_tuple_Daughter))\n",
        "\n",
        "    # Turns the tuples into an array to get the ratios which normalizes it\n",
        "    Parent_Ratios = np.array(summed_tuple_Parent) / np.array(summed_tuple_Parent).sum() if Normalization_Number_is_total_areas else np.array(summed_tuple_Parent) / Normalization_Number\n",
        "    Daughter_Ratios = np.array(summed_tuple_Daughter) / np.array(summed_tuple_Daughter).sum() if Normalization_Number_is_total_areas else np.array(summed_tuple_Daughter) / Normalization_Number\n",
        "\n",
        "    # Calculating the errors and alphas of the newly combined data\n",
        "    Alphas_of_Combined_Data = tuple(Daughter_Ratios / Parent_Ratios)\n",
        "    Enrichments_of_Combined_Data = tuple((Daughter_Ratios/Parent_Ratios) - 1)\n",
        "    Preferences = ['Parent' if ε < 0 else 'Neither' if ε == 0 else 'Daughter' for ε in Enrichments_of_Combined_Data]\n",
        "    Errors_of_Combined_Data = tuple((Daughter_Ratios/Parent_Ratios) * np.sqrt(\n",
        "        (np.sqrt(np.array(summed_tuple_Daughter)) / np.array(summed_tuple_Daughter))**2 +\n",
        "        (np.sqrt(np.array(summed_tuple_Parent)) / np.array(summed_tuple_Parent))**2))\n",
        "\n",
        "    #Round off Data to the decimal place specified in Number_of_Decimals if Number_of_Decimals is an integer\n",
        "    if Number_of_Decimals is not None:\n",
        "      summed_tuple_Parent = tuple(round(value, Number_of_Decimals) for value in summed_tuple_Parent)\n",
        "      summed_tuple_Daughter = tuple(round(value, Number_of_Decimals) for value in summed_tuple_Daughter)\n",
        "      Combined_Error_Parent = np.round(Combined_Error_Parent, Number_of_Decimals)\n",
        "      Combined_Error_Daughter = np.round(Combined_Error_Daughter, Number_of_Decimals)\n",
        "      Parent_Ratios = np.round(Parent_Ratios, Number_of_Decimals)\n",
        "      Daughter_Ratios = np.round(Daughter_Ratios, Number_of_Decimals)\n",
        "      Alphas_of_Combined_Data = tuple(round(value, Number_of_Decimals) for value in Alphas_of_Combined_Data)\n",
        "      Enrichments_of_Combined_Data = tuple(round(value, Number_of_Decimals) for value in Enrichments_of_Combined_Data)\n",
        "      Errors_of_Combined_Data = tuple(round(value, Number_of_Decimals) for value in Errors_of_Combined_Data)\n",
        "\n",
        "    # Make the Table\n",
        "    Table_of_Ultimate_Power = PDF_MAKER.define_table(custom_labels, np.array(['α', 'ε', 'Parent Error', 'Daughter Error', 'Error Bar', 'Preference']), np.array([Alphas_of_Combined_Data, Enrichments_of_Combined_Data, Combined_Error_Parent, Combined_Error_Daughter, Errors_of_Combined_Data, Preferences]), pivot = Pivot)\n",
        "    if save_Table:\n",
        "      Current_Table_Title = \"Integrated Overview of All Data linked to the Same Isolation\"\n",
        "      PDF_MAKER.add_table(Table_of_Ultimate_Power, title  = Current_Table_Title, Additional_Information = additional_Information)\n",
        "    if save_Table:\n",
        "      return Table_of_Ultimate_Power\n",
        "\n",
        "def overlaid_line_plot_EVI(save_pdf = False):\n",
        "    \"\"\"Makes an graph of all enrichment vs isotope data. This assumes you have used all_calculations_and_graphs multiple times or calculate_δα_indv multiple times and now want to plot all the data\n",
        "    Put save_pdf = True if you are in the process of adding this chart to a pdf\"\"\"\n",
        "    for key in All_enrichments:\n",
        "        plt.errorbar(All_Isotopes, All_enrichments[key], yerr = All_Errors[key], label=key, capsize=3, marker='o', linestyle='-')\n",
        "    plt.xlabel('Isotope')\n",
        "    plt.ylabel('ε')\n",
        "    plt.title(\"ε Trends vs. Isotopic Composition for Each Date\")\n",
        "    plt.legend(title = \"Dates\", fontsize='small', borderpad=0.3, labelspacing=0.2, loc='best')\n",
        "    plt.grid(True)\n",
        "    custom_labels = []\n",
        "    for isotope in All_Isotopes:\n",
        "      isotope_label = f\"$\\\\mathrm{{^{{{isotope}}}{Atom_Names}}}$\"\n",
        "      custom_labels.append(isotope_label)  # Custom labels\n",
        "    x_axis = All_Isotopes\n",
        "    plt.xticks(x_axis, custom_labels)\n",
        "    if not save_pdf:\n",
        "       plt.show()\n",
        "\n",
        "def overlaid_line_plot_AVI(save_pdf = False):\n",
        "    \"\"\"Makes an graph of all alpha vs isotope data. This assumes you have used all_calculations_and_graphs multiple times or calculate_δα_indv multiple times and now want to plot all the data\n",
        "    Put save_pdf = True if you are in the process of adding this chart to a pdf\"\"\"\n",
        "    for key in All_alphas:\n",
        "        plt.errorbar(All_Isotopes, All_alphas[key], yerr = All_Errors[key], label=key, capsize=3, marker='o', linestyle='-')\n",
        "    plt.xlabel('Isotope')\n",
        "    plt.ylabel('α')\n",
        "    plt.title(\"α Coefficient Trends as a Function of Isotopic Composition for Each Date\")\n",
        "    plt.legend(title = \"Dates\", fontsize='small', borderpad=0.3, labelspacing=0.2, loc='best')\n",
        "    plt.grid(True)\n",
        "    custom_labels = []\n",
        "    for isotope in All_Isotopes:\n",
        "      isotope_label = f\"$\\\\mathrm{{^{{{isotope}}}{Atom_Names}}}$\"\n",
        "      custom_labels.append(isotope_label)  # Custom labels\n",
        "    x_axis = All_Isotopes\n",
        "    plt.xticks(x_axis, custom_labels)\n",
        "    if not save_pdf:\n",
        "      plt.show()\n",
        "\n",
        "def all_calculations_and_graphs(molecule_name, name_of_atom_of_interest ,date_data_was_taken, molecular_weight, AMU, all_file_path, info_on_file_path,\n",
        "                                row_starts_and_ends_parent, row_starts_and_ends_daughter, Bin_size, number_of_peaks, Normalization_Number,\n",
        "                                parent_daughter, ROWSKIP = 7,  Drop_files = None, Print_Info = True, Print_Warnings = True, Print_Notices = True, Print_Errors = True, professional = False,\n",
        "                                Normalization_Number_is_total_areas = False, rows_are_masses = False, graphs = False, save_PDF_Graph = False, save_PDF_Table = False, PDF_MAKER = None, Number_of_Decimals = None, Separate_files = False,\n",
        "                                file_path_for_separate_file = None, separate_file_info_on_file_path = None, Debug = False):\n",
        "    \"\"\"This function will produce all the intended graphs and perform the calculations needed to generate graphs listed within this function for data taken on the same day. You'd use this multiple times\n",
        "    to add data to the global variables then use the functions above to generate graphs for all data all together. These are graphs where you can only make graphs under the same isolation. To use the charts that\n",
        "    use data from multiple different isolations, you'll need to use\n",
        "\n",
        "    What this function does and explanation of the arguments:\n",
        "\n",
        "    It first makes the class instance, then takes a string of the molecule name, a string of the name of the atom you're looking at, a string of the data it was taken, an integer of the molecular weight\n",
        "    of the molecule, and an integer value of the AMU of the atom, and defines it for later use. Next, it takes the string of the file_path that has all the data files and takes the string of the link to a public google sheets\n",
        "    or other method of getting the information pertaining to the data and defines the characteristics of that data. Third, it takes the rows and ends of parents and daughters in lists of where each peak starts and ends\n",
        "    (can be only one list if the data is evenly spaced by bins), an integer of the bin size of how many rows from row starts (inclusive) it'll inlcude to row end (inclusive), an integer of the number_of peaks you are including in the\n",
        "    calcultions, a integer Normalization_Number to what you'd want to normalize the data to (1 if none), parent_daughter which is the two NCE values you're comparing as a string in a list ['parent','daughter'], and ROWSKIP which\n",
        "    you can specify to be equal to the amount of rows you want to skip to get to the headers of the data in the event that your data doesn't start at the first row (in this function you don't need to account for any\n",
        "    indexing differences, just put the rows as they appear in the documents based on indexes or masses depending). Additionally, if you have bad data files you can always specify which files to drop\n",
        "     like {'0': [1,2], '70': [8,9]} which works even if theres only one specified. You can also specify Print_Info = False if you do not want to print out every final calculation, Print_Warnings = False if you want to not\n",
        "    print out any warnings (Things that do not cause errors, but may cause an error in later functions), Print_Errors = False if you don't want to print any errors (Things that do not work and break functionality due to inputs),\n",
        "    Print_Notices = False if you do not want to print any notices (things that do not cause errors or require a warning, but may be unintended on the Users part), professional = True if you want the graphs to not have color themes and all be the same color,\n",
        "    Normalization_Number_is_total_areas = True (if you are normalizing by total area then this will automatically normalize it by this for you), rows_are_masses = True (if you want to bin by starting masses and ending masses instead\n",
        "    of indexing by row), and graphs = False (if you don't want to make any graphs).\n",
        "\n",
        "    The PDF_MAKER argument is for an instance of MakePDF class of which you'd save any data to such as tables or graphs. If save_PDF_Graph or Save_PDF_Table is True, the graphs and/or tables will save respectively. For this function,\n",
        "    it will output the tables and graphs I set it to within the function, for more customization you'll have to change it yourself. You can also specify to what decimal place the values will be rounded with Number_of_Decimals = integer of how many decimals you want in the tables.\n",
        "\n",
        "    In the event you need to use calculate_δα_indv, but your Parent and Daughter files are in seperate folders with different NCE_Values, then you'll have to Put Seperate_files = True and include the new file path to the folder containing said value in file_path_for_seperate_file. Additionally, recall\n",
        "    that if it's not the first and only instance of that NCE you'll need to also specify the isolation in ['Parent', 'Daughter'] like '20_391(28)' for example.\n",
        "    You'll also need to provide a google_sheet/spreadsheet in the same format as before for the seperate_file in seperate_file_info_on_file_path.\n",
        "\n",
        "    Set Debug = True to run any information during intermediate steps meant for debugging.\n",
        "\n",
        "    The names can be edited in multiple ways such as making the data just a name if you want to manually take it apart for specific analyses, you can just copy paste\n",
        "    the workflow and fill out the data and change variable names. Everything is also specified in more detail in the actual functions themselves.\n",
        "\n",
        "    NOTE: There is a case where you'd need to do something easy that I did not implement. When you run this function it stores total areas in the global variable for future use with a key being the dates the data was taken.\n",
        "    If you took data for parent or daughter on multiple days and the parent or daughter (whichever you didn't use first) is less than or greater than the number of days you took data,\n",
        "    you'd still need to have a parent and daughter for this function to work. It would have repeats of the same data. In this event, you should manually clear the global variable for parent or daughter masses and total areas\n",
        "    after using this function n amount of times, then run the function after you have an m number of files. By this I mean if you have 3 days where you took the data relating to the parent data and only took data for this daughter on one day, you'll\n",
        "    run the function with separate files arguments twice, then set All_Daughter_Areas = {} and Daughter_Masses = {}. After this you run the last function still with the separate files arguments and now you'll\n",
        "    have the data for the parent NCE from the three seperate days and the daughter NCE from the one day for data analysis. To reiterate, in this case n = 3 and m = 1, you clear the respective global variable when m = n dates.\n",
        "\n",
        "    For further clarification, I am defining Error, Warning, and Notice as follows:\n",
        "    Error: Anything that breaks the function it is in, where it would not be able to continue so it stops where the Error occurs\n",
        "    Warning: Anything that does not break the function it is in, but it will likely break a diffrent function that relies on anything that is changed or outputted within that function\n",
        "    Notice: Anything that does not break the function it is in and will likely not break any of the other functions, but may be unintended by the user that this occured or it won't function as intended due to it\n",
        "    All of these assume that there is an issue with what the User puts in and not the functions themselves.\n",
        "\n",
        "    Quick Guide to Arguments:\n",
        "    molecule_name: A string of the name of the molecule. It will be formatted if entered a format like Nd(NO_{3})_{4}^{-}.\n",
        "    name_of_atom_of_interest: A string of the name of the Atom you're observing, so for Nd(NO_{3})_{4}^{-} this would be 'Nd'\n",
        "    date_data_was_taken: A string of the date you took the data in the format of 'MM/DD/YYYY'. If the Parent and Daughter were taken on seperate dates, you can enter both dates or only one.\n",
        "    molecular_weight: An integer containing the molecular weight of the molecule before it was broken up in a mass spectrometer.\n",
        "    AMU: An integer of mass in AMU for your atom_of_interest\n",
        "    all_file_path: A string containing the path that takes you to the collected data. Formatting is important for this function, so ensure the columns have the specified names used in other functions.\n",
        "    info_on_file_path: A string of the path that takes you to a googlesheet (change function if spreadsheet) containing the information formatted in the specified format in other functions.\n",
        "    row_starts_and_ends_parent. A list of lists containg the index of the rows where you want to start collecting data and stop collection data such as [[start_1,end_1], [start_2,end_2]] for the 'Parent' molecule\n",
        "    row_starts_and_ends_daughterA list of lists containg the index of the rows where you want to start collecting data and stop collection data such as [[start_1,end_1], [start_2,end_2]] for the 'Daughter' molecule\n",
        "    Bin_size: A integer of how many rows will be included in one bin, you should account for this in your row_starts_and_ends.\n",
        "    number_of_peaks: An integer of how many peaks are you looking at in the spectrum that you are including in the data (account for this in Bin_size and row_starts_and_ends.)\n",
        "    Normalization_Number: An inyeger of what you want to normalize the peaks by, if you choose to normalize by total areas then this can be an arbitary integer and you set the optional argument to True\n",
        "    parent_daughter: An list containing two strings containing what the parent and daughter NCE values are. For example, ['0','70']\n",
        "    ROWSKIP: An integer of how many rows you'd need to skip in all_data_files to get to the header of the data. If none then put zero.\n",
        "\n",
        "    Optional Arguments:\n",
        "    Drop_files = None # Put in a dictionary with keys and and lists of what file numbers to drop\n",
        "    Print_Info = True # To print calculations and general information\n",
        "    Print_Warnings = True # Print anything that may cause Errors later\n",
        "    Print_Notices = True # Print any information someone may want to know that isn't causing any errors, but may be an unintended use of the function on the users part\n",
        "    Print_Errors = True\n",
        "    professional = True\n",
        "    Normalization_Number_is_total_areas = False\n",
        "    rows_are_masses = False\n",
        "    graphs = False # Show the graphs you generate. Otherwise it'll skip it\n",
        "    save_PDF_Graph = False\n",
        "    save_PDF_Table = False\n",
        "    PDF_MAKER = None\n",
        "    Number_of_Decimals = None\n",
        "    Seperate_files = False.  If you want an NCE value that's in a different folder, set as True\n",
        "    file_path_for_seperate_file = None\n",
        "    seperate_file_info_on_file_path = None\n",
        "    Debug = False\n",
        "\n",
        "   \"\"\"\n",
        "\n",
        "    MSD = Mass_Spectrometer_Data() # Initialize a Mass_Spectrometer_Data class\n",
        "    # Any attribute in MSD will be copied by The_Method class instance that is made in the Error_of_Data instance\n",
        "    MSD.warnings = Print_Warnings # True by default\n",
        "    MSD.notices = Print_Notices # True by default\n",
        "    MSD.errors = Print_Errors # True by default\n",
        "    MSD.define_molecule_characteristics(molecule_name, name_of_atom_of_interest ,date_data_was_taken, molecular_weight, AMU) # Define molecule characteristics\n",
        "    if Separate_files:\n",
        "      MSD_2 = Mass_Spectrometer_Data() # Make a second instance of this data class to get the files for the NCE in a Seperate_File\n",
        "      MSD_2.warnings = Print_Warnings # True by default\n",
        "      MSD_2.notices = Print_Notices # True by default\n",
        "      MSD_2.errors = Print_Errors # True by default\n",
        "      #Ensuring That MSD and MSD_2 Functions without Breaking due to missing values for the other NCE.\n",
        "      MSD.define_all_file_path(all_file_path) # Gather the individual file paths from the path to the folder with all the files\n",
        "      MSD_2.define_all_file_path(file_path_for_separate_file) # Gather the individual file paths from the path to the folder with all the files\n",
        "      MSD.define_NCE(info_on_file_path, GoogleSheets = True) # Change this if it's not true for you\n",
        "      MSD_2.define_NCE(separate_file_info_on_file_path, GoogleSheets = True) # Change this if it's not true for you\n",
        "      MSD.only_include_CSV_files() # Only gathers CSV files\n",
        "      MSD_2.only_include_CSV_files() # Only gathers CSV files\n",
        "      MSD.define_NCE_Labels_and_Associated_Files() # Gathers the NCE labels from the google sheet/other spreadsheet and associates them to the file paths by indexes\n",
        "      MSD_2.define_NCE_Labels_and_Associated_Files() # Gathers the NCE labels from the google sheet/other spreadsheet and associates them to the file paths by indexes\n",
        "      MSD.get_file_paths()\n",
        "      MSD_2.get_file_paths()\n",
        "      try:\n",
        "          # Case where Parent key is in MSD and Daughter key is in MSD_2. It identifies this by checking the specific NCE_ID made the define_NCE function in the MSD class.\n",
        "          # Separate files will assume that the specific NCE wouldn't be in both as that is its purpose. There is no print but this would be self.warnings if it were in both since this works if NCE_ID is in MSD.\n",
        "          MSD_KEY = parent_daughter[0] # Assumes MSD holds parent key\n",
        "          MSD_2_KEY = parent_daughter[1] # Assumes MSD_2 holds daughter key\n",
        "          Error = False # True if Parent key is not in MS\n",
        "          # Compare Parent key to NCE_IDs and NCE. If it were '0' that means it's the first instance of '0' otherwise it'd be '0_391(28)', for example, which is 'NCE_ISOLATION'. However both files could have a zero, so isolation may matter\n",
        "          if MSD_KEY in MSD.NCE: # Check if the key is in the NCE values\n",
        "            if MSD_KEY in MSD_2.NCE: # Check if the NCE value also happens to be in MSD_2. If this is the case we have to check the isolation with the NCE value stored in the IDs\n",
        "              # At this point in the code we can see that MSD 2 and MSD 1 have an NCE value like '0' with no specific isolation specified. So now we need to see if '0' for Parent key exists in MSD\n",
        "              Index = MSD.NCE.index(MSD_KEY) # Get the appropriate index\n",
        "              if MSD_KEY +\"_\"+ MSD.Isolations[Index] == MSD.nce_id[Index]: # Check if the isolation of the Key matches the NCE_id made using MSD instance data.\n",
        "                pass # The Parent key is in MSD.NCE values and the isolation matches the NCE_ISOLATION IDs created for MSD. Therefore we can proceed with this try block\n",
        "              else:\n",
        "                Error = True # The Parent key is in MSD.NCE values, but after we compared the NCE_ID we find that they are not the same Isolation. So this means the parent key is in MSD_2\n",
        "            else:\n",
        "              pass # The Parent key is in MSD.NCE, but not MSD_2.NCE, so the Parent key is in MSD and we can continue with the try block\n",
        "          else:\n",
        "              Error = True # The Parent key is not in MSD.NCE values in any form\n",
        "          if Error:\n",
        "              raise ValueError(\"Parent key is not in MSD.NCE values\")\n",
        "          print('Parent NCE value is in the first folder, the daughter NCE value is in the separate folder') if Print_Notices or Debug else None\n",
        "          if Drop_files is not None:\n",
        "            if MSD_KEY in Drop_files:\n",
        "              MSD.drop_files({MSD_KEY: Drop_files[MSD_KEY]})\n",
        "            if MSD_2_KEY in Drop_files:\n",
        "              MSD_2.drop_files({MSD_2_KEY: Drop_files[MSD_2_KEY]})\n",
        "          if Debug:\n",
        "            print(f\"NCE for MSD before combining it: {MSD.NCE}\")\n",
        "            print(f\"NCE for MSD_2 before combining it: {MSD_2.NCE}\")\n",
        "            print(f\"numeric_file_indices for MSD beofre combining: {MSD.numeric_file_indices}\")\n",
        "            print(f\"numeric_file_indices for MSD_2 beofre combining: {MSD_2.numeric_file_indices}\")\n",
        "            print(f\"File names in MSD.file_names before combining: {MSD.file_names}\")\n",
        "            print(f\"File names in MSD_2.file_names before combining: {MSD_2.file_names}\")\n",
        "            print(f\"Dictionary with NCE keys and corresponding files in MSD before combining: {MSD.NCE_Labels_and_Files}\")\n",
        "            print(f\"Dictionary with NCE keys and corresponding files in MSD_2 before combining: {MSD_2.NCE_Labels_and_Files}\")\n",
        "            print(f\"The Dictionary containing the NCE keys and their corresponding file paths in MSD before combination: {MSD.NCE_File_Paths}\")\n",
        "            print(f\"The Dictionary containing the NCE keys and their corresponding file paths in MSD_2 before combination: {MSD_2.NCE_File_Paths}\")\n",
        "          # Check and remove duplicates from MSD_2 before merging. We have confirmed the parent NCE is in MSD, but if there were a key of the same NCE and isolation in MSD 2, it would cause errors. To deal with this we do a check to remove the shared key from the daughter dictionary\n",
        "\n",
        "          # Removes any similar keys from other since it would lead to duplications. However we also want to remove MSD_2 from MSD for that reason as if we didn't we'd get the wrong values when we combine them\n",
        "          # Remove entire key from MSD_2.numeric_file_indices_dict if key exists in MSD.numeric_file_indices_dict, but keep the specified key MSD_2_KEY\n",
        "          for key in list(MSD_2.numeric_file_indices_dict.keys()):\n",
        "              if key in MSD.numeric_file_indices_dict and key != MSD_2_KEY:\n",
        "                  print(f\"Removing duplicate key '{key}' from MSD_2.numeric_file_indices_dict\") if Debug else None\n",
        "                  # Remove the key from MSD_2\n",
        "                  del MSD_2.numeric_file_indices_dict[key]\n",
        "\n",
        "                  # Also remove MSD_2_KEY from MSD if it exists\n",
        "                  if MSD_2_KEY in MSD.numeric_file_indices_dict:\n",
        "                      print(f\"Also removing {MSD_2_KEY} from MSD.numeric_file_indices_dict\") if Debug else None\n",
        "                      del MSD.numeric_file_indices_dict[MSD_2_KEY]\n",
        "\n",
        "          # Remove entire key from MSD_2.NCE_Labels_and_Files if key exists in MSD.NCE_Labels_and_Files, but keep the specified key MSD_2_KEY\n",
        "          for key in list(MSD_2.NCE_Labels_and_Files.keys()):\n",
        "              if key in MSD.NCE_Labels_and_Files and key != MSD_2_KEY:\n",
        "                  print(f\"Removing duplicate key '{key}' from MSD_2.NCE_Labels_and_Files\") if Debug else None\n",
        "                  # Remove the key from MSD_2\n",
        "                  del MSD_2.NCE_Labels_and_Files[key]\n",
        "\n",
        "                  # Also remove MSD_2_KEY from MSD if it exists\n",
        "                  if MSD_2_KEY in MSD.NCE_Labels_and_Files:\n",
        "                      print(f\"Also removing {MSD_2_KEY} from MSD.NCE_Labels_and_Files\") if Debug else None\n",
        "                      del MSD.NCE_Labels_and_Files[MSD_2_KEY]\n",
        "\n",
        "          # Remove entire key from MSD_2.NCE_File_Paths if key exists in MSD.NCE_File_Paths, but keep the specified key MSD_2_KEY\n",
        "          for key in list(MSD_2.NCE_File_Paths.keys()):\n",
        "              if key in MSD.NCE_File_Paths and key != MSD_2_KEY:\n",
        "                  print(f\"Removing duplicate key '{key}' from MSD_2.NCE_File_Paths\") if Debug else None\n",
        "                  # Remove the key from MSD_2\n",
        "                  del MSD_2.NCE_File_Paths[key]\n",
        "\n",
        "                  # Also remove MSD_2_KEY from MSD if it exists\n",
        "                  if MSD_2_KEY in MSD.NCE_File_Paths:\n",
        "                      print(f\"Also removing {MSD_2_KEY} from MSD.NCE_File_Paths\") if Debug else None\n",
        "                      del MSD.NCE_File_Paths[MSD_2_KEY]\n",
        "\n",
        "          # Merging MSD.NCE_Labels_and_Files\n",
        "          merged_dict = {}\n",
        "          for key in set(MSD.NCE_Labels_and_Files) | set(MSD_2.NCE_Labels_and_Files):  # Union of keys. Which is fine because they should be the exact same isolation\n",
        "              if key in MSD.NCE_Labels_and_Files and key in MSD_2.NCE_Labels_and_Files:\n",
        "                  # Merge and flatten the values\n",
        "                  merged_dict[key] = MSD.NCE_Labels_and_Files[key] + MSD_2.NCE_Labels_and_Files[key]\n",
        "              else:\n",
        "                  # Take the value from whichever dictionary has the key\n",
        "                  merged_dict[key] = MSD.NCE_Labels_and_Files.get(key, MSD_2.NCE_Labels_and_Files.get(key))\n",
        "          MSD.NCE_Labels_and_Files = merged_dict\n",
        "          print(f\"MSD.NCE_Labels_and_Files: {MSD.NCE_Labels_and_Files}\") if Debug else None\n",
        "\n",
        "          # Merging MSD.NCE_File_Paths\n",
        "          merged_dict = {} # Clear merged_dict from previous function\n",
        "          for key in set(MSD.NCE_File_Paths) | set(MSD_2.NCE_File_Paths):  # Union of keys. As they have to be the same isolation so all files are valid to be put under the same key.\n",
        "              if key in MSD.NCE_File_Paths and key in MSD_2.NCE_File_Paths:\n",
        "                  # Merge and flatten the values\n",
        "                  merged_dict[key] = MSD.NCE_File_Paths[key] + MSD_2.NCE_File_Paths[key]\n",
        "              else:\n",
        "                  # Take the value from whichever dictionary has the key\n",
        "                  merged_dict[key] = MSD.NCE_File_Paths.get(key, MSD_2.NCE_File_Paths.get(key))\n",
        "          MSD.NCE_File_Paths = merged_dict\n",
        "          merged_numeric_indices = {}\n",
        "          for key in set(MSD.numeric_file_indices_dict) | set(MSD_2.numeric_file_indices_dict):  # Union of keys\n",
        "              if key in MSD.numeric_file_indices and key in MSD_2.numeric_file_indices:\n",
        "                # Merge and flatten the values\n",
        "                merged_numeric_indices[key] = MSD.numeric_file_indices_dict[key] + MSD_2.numeric_file_indices_dict[key]\n",
        "              else:\n",
        "                # Take the value from whichever dictionary has the key\n",
        "                merged_numeric_indices[key] = MSD.numeric_file_indices_dict.get(key, MSD_2.numeric_file_indices_dict.get(key))\n",
        "          MSD.numeric_file_indices_dict = merged_numeric_indices\n",
        "          MSD.numeric_file_indices = MSD.numeric_file_indices_dict[MSD_KEY] + MSD.numeric_file_indices_dict[MSD_2_KEY]\n",
        "          print(f\"new numeric file indices: {MSD.numeric_file_indices}\") if Debug else None\n",
        "          print(f\"Combined NCE File Path dictionary: {MSD.NCE_File_Paths}\") if Debug else None\n",
        "      except:\n",
        "          print('Parent NCE value is in in the separate folder') if Print_Notices or Debug else None\n",
        "          # Case where Daughter key is in MSD and Parent key is in MSD_2\n",
        "          MSD_KEY = parent_daughter[1]\n",
        "          MSD_2_KEY = parent_daughter[0]\n",
        "          if Drop_files is not None:\n",
        "            if MSD_KEY in Drop_files:\n",
        "              MSD.drop_files({MSD_KEY: Drop_files[MSD_KEY]})\n",
        "            if MSD_2_KEY in Drop_files:\n",
        "              MSD_2.drop_files({MSD_2_KEY: Drop_files[MSD_2_KEY]})\n",
        "          if Debug:\n",
        "            print(f\"NCE for MSD before combining it: {MSD.NCE}\")\n",
        "            print(f\"NCE for MSD_2 before combining it: {MSD_2.NCE}\")\n",
        "            print(f\"numeric_file_indices for MSD before combining: {MSD.numeric_file_indices}\")\n",
        "            print(f\"numeric_file_indices for MSD_2 beofre combining: {MSD_2.numeric_file_indices}\")\n",
        "            print(f\"File names in MSD.file_names before combining: {MSD.file_names}\")\n",
        "            print(f\"File names in MSD_2.file_names before combining: {MSD_2.file_names}\")\n",
        "            print(f\"Dictionary with NCE keys and corresponding files in MSD before combining: {MSD.NCE_Labels_and_Files}\")\n",
        "            print(f\"Dictionary with NCE keys and corresponding files in MSD_2 before combining: {MSD_2.NCE_Labels_and_Files}\")\n",
        "            print(f\"The Dictionary containing the NCE keys and their corresponding file paths in MSD before combination: {MSD.NCE_File_Paths}\")\n",
        "            print(f\"The Dictionary containing the NCE keys and their corresponding file paths in MSD_2 before combination: {MSD_2.NCE_File_Paths}\")\n",
        "          # Check and remove duplicates from MSD_2 before merging. Although we confirmed parent is in MSD_2, there is still the possibility of the same NCE and isolation in MSD which would cause errors. To deal with this we do a check to that key from the daughter dictionary\n",
        "\n",
        "          # Removes any similar keys from other since it would lead to duplications. However we also want to remove MSD from MSD_2 for that reason as if we didn't we'd get the wrong values when we combine them\n",
        "          # Remove entire key from MSD.numeric_file_indices_dict if key exists in MSD_2.numeric_file_indices_dict, but keep the specified key MSD_KEY\n",
        "          for key in list(MSD.numeric_file_indices_dict.keys()):\n",
        "              if key in MSD_2.numeric_file_indices_dict and key != MSD_KEY:\n",
        "                  print(f\"Removing duplicate key '{key}' from MSD.numeric_file_indices_dict\") if Debug else None\n",
        "                  # Remove the key from MSD\n",
        "                  del MSD.numeric_file_indices_dict[key]\n",
        "\n",
        "                  # Also remove MSD_KEY from MSD_2 if it exists\n",
        "                  if MSD_KEY in MSD_2.numeric_file_indices_dict:\n",
        "                      print(f\"Also removing {MSD_KEY} from MSD_2.numeric_file_indices_dict\") if Debug else None\n",
        "                      del MSD_2.numeric_file_indices_dict[MSD_KEY]\n",
        "\n",
        "          # Remove entire key from MSD.NCE_Labels_and_Files if key exists in MSD_2.NCE_Labels_and_Files, but keep the specified key MSD_KEY\n",
        "          for key in list(MSD.NCE_Labels_and_Files.keys()):\n",
        "              if key in MSD_2.NCE_Labels_and_Files and key != MSD_KEY:\n",
        "                  print(f\"Removing duplicate key '{key}' from MSD.NCE_Labels_and_Files\") if Debug else None\n",
        "                  # Remove the key from MSD\n",
        "                  del MSD.NCE_Labels_and_Files[key]\n",
        "\n",
        "                  # Also remove MSD_KEY from MSD_2 if it exists\n",
        "                  if MSD_KEY in MSD_2.NCE_Labels_and_Files:\n",
        "                      print(f\"Also removing {MSD_KEY} from MSD_2.NCE_Labels_and_Files\") if Debug else None\n",
        "                      del MSD_2.NCE_Labels_and_Files[MSD_KEY]\n",
        "\n",
        "          # Remove entire key from MSD.NCE_File_Paths if key exists in MSD_2.NCE_File_Paths, but keep the specified key MSD_KEY\n",
        "          for key in list(MSD.NCE_File_Paths.keys()):\n",
        "              if key in MSD_2.NCE_File_Paths and key != MSD_KEY:\n",
        "                  print(f\"Removing duplicate key '{key}' from MSD.NCE_File_Paths\") if Debug else None\n",
        "                  # Remove the key from MSD\n",
        "                  del MSD.NCE_File_Paths[key]\n",
        "\n",
        "                  # Also remove MSD_KEY from MSD_2 if it exists\n",
        "                  if MSD_KEY in MSD_2.NCE_File_Paths:\n",
        "                      print(f\"Also removing {MSD_KEY} from MSD_2.NCE_File_Paths\") if Debug else None\n",
        "                      del MSD_2.NCE_File_Paths[MSD_KEY]\n",
        "\n",
        "          # Merging MSD_2.NCE_File_Paths\n",
        "          merged_dict = {}\n",
        "          for key in set(MSD_2.NCE_File_Paths) | set(MSD.NCE_File_Paths):  # Union of keys\n",
        "              if key in MSD_2.NCE_File_Paths and key in MSD.NCE_File_Paths:\n",
        "                  # Merge and flatten the values\n",
        "                  merged_dict[key] = MSD_2.NCE_File_Paths[key] + MSD.NCE_File_Paths[key]\n",
        "              else:\n",
        "                  # Take the value from whichever dictionary has the key\n",
        "                  merged_dict[key] = MSD_2.NCE_File_Paths.get(key, MSD.NCE_File_Paths.get(key))\n",
        "          MSD.NCE_File_Paths = merged_dict\n",
        "\n",
        "          # Merging Numeric_File_Indices\n",
        "          merged_numeric_indices = {}\n",
        "          for key in set(MSD_2.numeric_file_indices_dict) | set(MSD.numeric_file_indices_dict):  # Union of keys\n",
        "              if key in MSD_2.numeric_file_indices and key in MSD.numeric_file_indices:\n",
        "                # Merge and flatten the values\n",
        "                merged_numeric_indices[key] = MSD_2.numeric_file_indices_dict[key] + MSD.numeric_file_indices_dict[key]\n",
        "              else:\n",
        "                # Take the value from whichever dictionary has the key\n",
        "                merged_numeric_indices[key] = MSD_2.numeric_file_indices_dict.get(key, MSD.numeric_file_indices_dict.get(key))\n",
        "          MSD.numeric_file_indices_dict = merged_numeric_indices\n",
        "          MSD.numeric_file_indices = MSD.numeric_file_indices_dict[MSD_2_KEY] + MSD.numeric_file_indices_dict[MSD_KEY]\n",
        "          print(f\"new numeric file indices: {MSD.numeric_file_indices}\") if Debug else None\n",
        "          print(f\"Combined NCe file path dictionary: {MSD.NCE_File_Paths}\") if Debug else None\n",
        "    else:\n",
        "      # All Files for specified NCEs are in the same folder\n",
        "      print('Parent and Daughter NCE values are in the same folder') if Print_Notices or Debug else None\n",
        "      MSD.define_all_file_path(all_file_path) # Gather the individual file paths from the path to the folder with all the files\n",
        "      MSD.define_NCE(info_on_file_path, GoogleSheets = True) # Change this if it's not true for you\n",
        "      MSD.only_include_CSV_files() # Only gathers CSV files\n",
        "      MSD.define_NCE_Labels_and_Associated_Files() # Gathers the NCE labels from the google sheet/other spreadsheet and associates them to the file paths by indexes\n",
        "      if Drop_files is not None:\n",
        "          MSD.drop_files(Drop_files) # Drops files for any of the NCEs if specifed\n",
        "      MSD.get_file_paths() # Gathers all the file paths for the defined NCEs and makes a dictionary where each NCE has defined file paths\n",
        "    MSD.merge__and_store_files_by_nce(MSD.NCE) # Makes a big data frame by combining all data with same values together, but its use here is to ensure functionality as it has multiple uses and needs to be defined for some of the functions used here to work\n",
        "    Error_of_Data = Error_Propagation(MSD) # Initalize a Error class taking in the data from the MSD instance\n",
        "    Error_of_Data.print_information = Print_Info # True by default, will print out all calculations\n",
        "    if rows_are_masses: # If you chose to index by mass instead of the row indexes directly, otherwise it'll continue with indexing by rows\n",
        "      Error_of_Data.calculate_δα_indv(row_starts_and_ends_parent, row_starts_and_ends_daughter, Bin_size, number_of_peaks, Normalization_Number, parent_daughter, ROWSKIP = 7, NBTA = Normalization_Number_is_total_areas, rows_are_masses = True)\n",
        "    else:\n",
        "      Error_of_Data.calculate_δα_indv(row_starts_and_ends_parent, row_starts_and_ends_daughter, Bin_size, number_of_peaks, Normalization_Number, parent_daughter, ROWSKIP = 7, NBTA = Normalization_Number_is_total_areas)\n",
        "    if save_PDF_Table:\n",
        "      custom_labels = []\n",
        "      for isotope in All_Isotopes:\n",
        "        isotope_label = f\"$\\\\mathrm{{^{{{isotope}}}{Atom_Names}}}$\"\n",
        "        custom_labels.append(isotope_label)  # Custom labels for Isotopes\n",
        "      if Separate_files:\n",
        "        # Make title distinction because the default title assumes all the data was in the same folder taken on the same day\n",
        "        Current_Table_Title = f\"Tabulated Computational Outcomes for {MSD.molecule_name} from Data taken on Different Dates\"\n",
        "      else:\n",
        "        Current_Table_Title = f\"Tabulated Computational Outcomes for {MSD.molecule_name} Based on {MSD.date_data_was_taken} Data\"\n",
        "      PDF_MAKER.add_table(PDF_MAKER.define_table(custom_labels, np.array(['Alpha','Enrichment','Error Bar', 'Parent_Ratio', 'Daughter_Ratio']), [Error_of_Data.alphas, Error_of_Data.enrichments, Error_of_Data.δα, Error_of_Data.Parent_ratios, Error_of_Data.Daughter_ratios], pivot = True, number_of_decimals = Number_of_Decimals), title = Current_Table_Title,\n",
        "                          Additional_Information = f\"The NCEs are Parent: {parent_daughter[0]} {'|'} Daughter: {parent_daughter[1]}\") #Table totals\n",
        "    if graphs:\n",
        "      # To make the graphs if graphs is True\n",
        "      Graphing_Molecule_Data = Graphs(MSD, Error_of_Data) # Intalize a Graphing class that takes in the MSD and Error_of_Data instances\n",
        "      \"\"\"Graphing_Molecule_Data.professional = professional # False by default, will make all graphs the same color\n",
        "      Graphing_Molecule_Data.αVIs(save_pdf = save_PDF_Graph) # Create alpha versus isotope graph\n",
        "      Graphing_Molecule_Data.εVIs(save_pdf = save_PDF_Graph) # Create enrichment versus isotope graph\n",
        "      Graphing_Molecule_Data.RIVM()\"\"\" # Create Relative intensity versus masses graph\n",
        "      Graphing_Molecule_Data.generate_all_IVM(387.5,400,340,355,['0','70']) #You can change the bounds of the data with xlim left and right as well as change which NCE they are for within this function\n",
        "    elif save_PDF_Graph:\n",
        "      # Save the Graphs to the PDF if save_PDF_Graph is True. Same Descriptions as in Graphs\n",
        "      Graphing_Molecule_Data = Graphs(MSD, Error_of_Data) # Intalize a Graphing class that takes in the MSD and Error_of_Data instances\n",
        "      Graphing_Molecule_Data.professional = professional # False by default, will make all graphs the same color\n",
        "      PDF_MAKER.add_chart(Graphing_Molecule_Data.αVIs, save_pdf = save_PDF_Graph)\n",
        "      PDF_MAKER.add_chart(Graphing_Molecule_Data.εVIs, save_pdf = save_PDF_Graph)\n",
        "      Graphing_Molecule_Data.RIVM(PDF_Maker = PDF_MAKER)\n",
        "      #PDF_MAKER.add_chart(Graphing_Molecule_Data.generate_all_IVM, 387.5,400,340,355,['0','70'])\n",
        "\n",
        "def export_dataframe(data_frame, name):\n",
        "    \"\"\"Takes in any data frame or data you want to export and saves it in the file format you specify\"\"\"\n",
        "    filename = name + '.csv'\n",
        "    data_frame.to_csv(filename)\n",
        "    files.download(filename)\n",
        "\n",
        "#Classes\n",
        "class Mass_Spectrometer_Data:\n",
        "  \"\"\"Allows you to connect all the classes and data_files\"\"\"\n",
        "  def __init__(self):\n",
        "    self.molecule_name = \"[Dy(NO_{3})_{4}]^{-}\" #Example, the format doesn't matter, I just prefer it this way.\n",
        "    self.atom_name = 'Dy'\n",
        "    self.date_data_was_taken = \"11/14/1987\"\n",
        "    self.molecular_weight_of_molecule = 410.516 # g/mol. This is just an example\n",
        "    self.AMU_Element = 162.5\n",
        "    self.all_files_path = \"path\" # A string of the Path to folder containing every CSV file\n",
        "    self.info_on_files_path_or_URL = \"pathh\" # A string of the path to a CSV file of all info about the files. This is specific to the project. If google sheets,this should be a URL.\n",
        "    self.info_on_files_data = None\n",
        "    self.file_names = []\n",
        "    self.NCE = []\n",
        "    self.Isolations = []\n",
        "    self.NCE_Labels_and_Files = {} # A dictionary that shows which files belong to which NCE values. This does not specify isolation.\n",
        "    self.numeric_file_indices = []\n",
        "    self.NCE_File_Paths = {} # A dictionary containing the key of the NCE values and the value pair of a list of file path names corresponding to each file.\n",
        "    self.print_information = False # Set to True if you want to print out relevant information you have. This does not include error messages.\n",
        "    self.errors = True #Set to False if you don't want to print out any Errors.\n",
        "    self.warnings = True # Set to False if you don't want to print out any warnings.\n",
        "    self.notices = True # Set to False if you don't want to print out any notices\n",
        "    self.NCE_dataframes = {} # For the event that you want to just store all data into a data frame\n",
        "    self.nce_id = 'ISOUPTOPE' # A list of the unique NCE IDs\n",
        "    self.The_Lorax= [] # Has all the unique NCE IDs in the order they were found\n",
        "    self.unsorted_NCE = [] # A list of the NCE value for every file.\n",
        "    self.unsorted_Isolations = [] # A list of the Isolation for every file.\n",
        "    self.unsorted_NCE_ID = [] # A list of the NCE ID for every file.\n",
        "    self.numeric_file_indices_dict = {} # Puts the number of the files\n",
        "\n",
        "  def define_molecule_characteristics(self, molecule_name, name_of_atom_of_interest ,date_data_was_taken, molecular_weight, AMU):\n",
        "    \"\"\"Takes the of the molecule that you analyzed in the Mass spectrometer as a string, the name of the specific atom of the molecule that is of interest/all ligands are\n",
        "    bound to as a string, the date of the analysis in the format of MM/DD/YEAR as a string, the molecular weight of the molecule as a integer, and the AMU of the element\n",
        "    (such as Dy or Nd) from the periodic table as a integer.\"\"\"\n",
        "    try:\n",
        "        self.molecule_name = format_chemical_formula(molecule_name)\n",
        "    except:\n",
        "      self.molecule_name = molecule_name\n",
        "    self.atom_name = name_of_atom_of_interest\n",
        "    self.date_data_was_taken = date_data_was_taken\n",
        "    self.molecular_weight_of_molecule = molecular_weight\n",
        "    self.AMU_Element = AMU\n",
        "\n",
        "  def define_all_file_path(self, file_path):\n",
        "    \"\"\"Takes in a string of the file_path that contains every single file you want to use for data analysis. This path should contain every single CSV file you want to use\"\"\"\n",
        "    self.all_files_path = file_path\n",
        "\n",
        "\n",
        "  def define_NCE(self, info_on_files_path_or_URL, **Manually):\n",
        "    \"\"\"For this you can either put in a string of the file_path of a file with every NCE labled in a column with the associated file numbers\n",
        "     and let the code sort it out or add an additional arguments Manual = True and NCE_values = [put values in here]. The intended file_path has columns labeled\n",
        "     file number, isolation, NCE, and comment. file number is the number of the associated file such as file 0, file 1, etc. Isolation is the associated isolation\n",
        "     for that file in the format of isolated mass(bin width). NCE is the associated normalized collision energy used in the mass spectrometer. The comment is not\n",
        "     needed for the code, but you should examine it to deicde whether to that file later on in events such as running out of spray or forgetting to change the range you used;\n",
        "     the comment column is not needed for the code, but you could have one to decide whether the data isn't good to use for statisitcal analysis or not.\"\"\"\n",
        "\n",
        "    CSV, Excel, GoogleSheets, Manual = (Manually.get(key, False) for key in ['CSV', 'Excel', 'GoogleSheets', 'Manual']) # Extracts optional parameters or set default False values\n",
        "    self.info_on_files_path_or_URL = info_on_files_path_or_URL\n",
        "\n",
        "    # Load the appropriate file type\n",
        "    if CSV:\n",
        "        self.info_on_files_data = pd.read_csv(info_on_files_path_or_URL)\n",
        "    elif Excel:\n",
        "        self.info_on_files_data = pd.read_excel(info_on_files_path_or_URL)\n",
        "    elif GoogleSheets:\n",
        "        self.info_on_files_data = import_google_spreadsheet_file(info_on_files_path_or_URL)\n",
        "    else:\n",
        "        print('You need to specify what type of file it is (CSV, Excel, or GoogleSheets)') if self.errors else None\n",
        "        return  # Exit the function if file type is not specified\n",
        "    NCE_values = []\n",
        "    if Manual:  # If Manual is True, retrieve the provided NCE values\n",
        "        NCE_values = Manually.get('NCE_values', [])\n",
        "    else:\n",
        "        # Otherwise, extract NCE values from the data\n",
        "        data = self.info_on_files_data\n",
        "        nce_columns = {'NCE', 'nce', 'nce ', ' nce', 'NCE ', ' NCE'}\n",
        "        file_columns = {'file', 'file ', ' file', 'file #', 'file#', 'file number', 'File', 'file_number', 'File_Number', 'File_number', 'File_#', 'File#', 'File #', 'File ', ' File'}\n",
        "        isolations_and_width = {'Isolation', 'isolation', ' isolation', ' isolation', ' Isolation', 'Isolation '}\n",
        "        data.columns = ['NCE' if col in nce_columns else 'file number' if col in file_columns else 'isolation' if col in isolations_and_width else col\n",
        "        for col in data.columns]\n",
        "\n",
        "        if not any(col in data.columns for col in nce_columns) or not any(col in data.columns for col in file_columns) or not any(col in data.columns for col in isolations_and_width):\n",
        "            print(\"Error: 'NCE' or 'file number' or 'isolation' column missing from data.\") if self.error else None\n",
        "            return {'>:('}\n",
        "\n",
        "        self.info_on_files_data = data # To save the changed names for consistency\n",
        "        if 'NCE' in data.columns:\n",
        "            x = data['NCE'].tolist()\n",
        "            y = data['isolation'].tolist()\n",
        "            self.unsorted_NCE = x # To store NCE for each file so it can be used for indexing in different functions\n",
        "            self.unsorted_Isolations = y # To store NCE for each file so it can be used for indexing later in different functions\n",
        "            nce_id = [lambda t = t: str(x[t]) + \"_\" + str(y[t]) for t in range(len(x))]\n",
        "            nce_ids = [func() for func in nce_id] #NCE_IDs for every single file\n",
        "            self.unsorted_NCE_ID = nce_ids # To store NCE for each file so it can be used for indexing later in different functions\n",
        "            seen = set()\n",
        "            nce_ids = [x for x in nce_ids if not (x in seen or seen.add(x))] #Unique NCE_IDs\n",
        "            print(f\"All unique NCE and Isolations: {nce_ids}\") if self.print_information else None\n",
        "            # Ensures unique values while preserving order\n",
        "            The_Lorax = [] # Protects the unique NCE values like he protected the trees\n",
        "            Isolations = [] #Holds Isolations for any unique value\n",
        "            # If a NCE value is the first instance of itself, it'll be just that number. If it repeats and the isolations are not the same, it'll add a new NCE value as NCE_ISOLATION\n",
        "            for i in range(len(x)):\n",
        "                nce_isolation_id = str(x[i]) + \"_\" + str(y[i])  # Unique identifier as 'NCE_isolation'\n",
        "                if str(x[i]) not in NCE_values:\n",
        "                    NCE_values.append(str(x[i]))  # Append as a string\n",
        "                    The_Lorax.append(nce_isolation_id)\n",
        "                    Isolations.append(str(y[i]))\n",
        "                elif (str(x[i]) + ' ' + str(y[i])) not in NCE_values and nce_isolation_id not in The_Lorax:\n",
        "                      NCE_values.append(nce_isolation_id)\n",
        "                      The_Lorax.append(nce_isolation_id)\n",
        "                      Isolations.append(str(y[i]))\n",
        "            # Removes duplicate entries from the final list\n",
        "            NCE_values = list(dict.fromkeys(NCE_values))  # Keeps the order and removes duplicates\n",
        "\n",
        "    # Optionally print the NCE values if requested\n",
        "    print(f\"NCE Values: {NCE_values}\") if self.print_information else None\n",
        "    self.nce_id = nce_ids # NCE IDs\n",
        "    self.NCE = NCE_values # NCE Values where if it is the first instance of the NCE value it'll be that value, but if it's not it'll be NCE_ISOLATION. This also has no duplicates and is ordered in the order they're found\n",
        "    self.The_Lorax = The_Lorax # NCE IDs with preserved order and holds unique values only\n",
        "    self.Isolations = Isolations # Isolations for each Unique NCE value. This is ordered\n",
        "\n",
        "  def only_include_CSV_files(self):\n",
        "    \"\"\"This function may not be necessary for all, however if your folder contains other files that aren't CSV files of the data you want to use for the data analysis, this function\n",
        "    will sort through the files from self.all_files_path and know to only use the CSV files\"\"\"\n",
        "    try:\n",
        "            # List all files in the specified directory\n",
        "            all_files = os.listdir(self.all_files_path)\n",
        "\n",
        "            # Filter to include only CSV files\n",
        "            self.file_names = [file for file in all_files if file.endswith('.csv')]\n",
        "\n",
        "            # Optional: Print the list of included CSV files\n",
        "            print(\"Included CSV files:\", self.file_names) if self.print_information else None\n",
        "\n",
        "    except FileNotFoundError:\n",
        "            print(f\"The directory '{self.all_files_path}' does not exist.\") if self.errors else None\n",
        "    except Exception as womp:\n",
        "            print(f\"An error occurred: {womp}\") if self.warnings else None\n",
        "\n",
        "  def define_NCE_Labels_and_Associated_Files(self):\n",
        "    \"\"\"Associates file names with NCE values (0 or 70 or 0(isolation)) based on the 'NCE' column in the info_on_files_data.\n",
        "    Creates a numeric list from file names for proper indexing, ensuring that missing files are handled appropriately.\n",
        "    ensure the file is named in the following format: filename_####.csv, as to properly map file names to file numbers,\n",
        "    it'll drop the .csv and look at the digits after the _ .\"\"\"\n",
        "\n",
        "    # Create a numeric list from file names by extracting numbers before the first underscore\n",
        "    numeric_file_indices = []\n",
        "    for filename in self.file_names:\n",
        "        # Split the filename to extract the number before the underscore and remove '.csv'\n",
        "        num_part = filename.split('_')[1].replace('.csv', '') if '_' in filename else ''\n",
        "        # Convert to integer\n",
        "        numeric_file_indices.append(int(num_part))\n",
        "    self.numeric_file_indices = numeric_file_indices\n",
        "\n",
        "    # Initialize dictionaries for storing files based on plain NCE and formatted NCE\n",
        "    initial_nce_files = {nce: [] for nce in self.NCE}\n",
        "    Same_NCE_different_ISOLATION = []\n",
        "    for ID in self.nce_id:\n",
        "      if ID in self.NCE:\n",
        "        Same_NCE_different_ISOLATION.append(ID)\n",
        "    data = self.info_on_files_data\n",
        "    x = data['NCE'].tolist()\n",
        "    y = data['isolation'].tolist()\n",
        "\n",
        "   # Populate NCE_ID with files, using formatted `nce_key` as keys\n",
        "    for idx, row in self.info_on_files_data.iterrows():\n",
        "        nce_value = str(row['NCE'])\n",
        "        isolation = str(row['isolation'])\n",
        "        nce_key = f\"{nce_value}_{isolation}\"\n",
        "        file_number = int(row['file number'])\n",
        "\n",
        "        # Match file_number with numeric_file_indices and retrieve the filename\n",
        "        if file_number in numeric_file_indices:\n",
        "            index_in_list = numeric_file_indices.index(file_number)\n",
        "            filename = self.file_names[index_in_list]\n",
        "            if nce_key in Same_NCE_different_ISOLATION:\n",
        "                initial_nce_files[nce_key].append(filename)\n",
        "            else:\n",
        "                initial_nce_files[nce_value].append(filename)\n",
        "        else:\n",
        "            print(f\"Notice: File number {file_number} not found in defined file names.\") if self.notices else None\n",
        "\n",
        "    self.NCE_Labels_and_Files = initial_nce_files\n",
        "\n",
        "    # Create a dictionary for numeric file indices associated with each NCE key\n",
        "    numeric_indices_per_nce = {}\n",
        "    for nce_key, filenames in self.NCE_Labels_and_Files.items():\n",
        "        # Extract numeric indices for each file in the current NCE\n",
        "        numeric_indices = []\n",
        "        for filename in filenames:\n",
        "            # Extract the numeric part of the filename (after '_')\n",
        "            num_part = filename.split('_')[1].replace('.csv', '') if '_' in filename else ''\n",
        "            numeric_indices.append(int(num_part))\n",
        "        numeric_indices_per_nce[nce_key] = numeric_indices\n",
        "\n",
        "    self.numeric_file_indices_dict = numeric_indices_per_nce\n",
        "\n",
        "    # Optionally print the final result for verification\n",
        "    print(f\"NCE Labels and Files: {self.NCE_Labels_and_Files}\") if self.print_information else None\n",
        "\n",
        "    return self.NCE_Labels_and_Files\n",
        "\n",
        "  def drop_files(self, NCE_values_and_which_files_to_drop):\n",
        "    \"\"\"\n",
        "    Removes specified files from the dataset based on NCE values and file numbers provided in a dictionary format.\n",
        "    Args:\n",
        "        NCE_values_and_which_files_to_drop (dict): Dictionary where keys are NCE values (e.g., '0', '70')\n",
        "        and values are lists of file numbers to drop (e.g., {'0': [3, 6], '70': [5, 7]}).\n",
        "    \"\"\"\n",
        "    # Create a mapping from filename to the extracted file index\n",
        "    file_index_map = {file_name: int(file_name.split('_')[1].split('.')[0]) for file_name in self.file_names}\n",
        "\n",
        "    for NCE_value, file_numbers_to_drop in NCE_values_and_which_files_to_drop.items():\n",
        "        if NCE_value in self.NCE_Labels_and_Files:\n",
        "            # Convert file numbers to corresponding filenames based on the mapping\n",
        "            files_to_drop = [file_name for file_name, index in file_index_map.items() if index in file_numbers_to_drop]\n",
        "\n",
        "            # Filter out the files that are not in the drop list\n",
        "            updated_files = [file for file in self.NCE_Labels_and_Files[NCE_value] if file not in files_to_drop]\n",
        "\n",
        "            self.numeric_file_indices = [index for index in self.numeric_file_indices if index not in file_numbers_to_drop]\n",
        "\n",
        "            # Update the dictionary directly\n",
        "            self.NCE_Labels_and_Files[NCE_value] = updated_files\n",
        "\n",
        "            if self.print_information:\n",
        "                print(f\"Updated NCE {NCE_value}: {updated_files}\")\n",
        "        else:\n",
        "            print(f\"NCE {NCE_value} not found in the dictionary.\") if self.warnings else None\n",
        "\n",
        "    \"\"\"Test Cases and Workflow for clarity:\n",
        "\n",
        "    # First the class would be initialized\n",
        "    obj = Mass_Spectrometer_Data()\n",
        "\n",
        "    Then lets say you ended up with self.NCE_Labels_and_Files = {\n",
        "            '0': [1, 2, 3, 4, 5, 6],\n",
        "            '70': [10, 20, 30, 40],\n",
        "            '90': [100, 200, 300]}\n",
        "\n",
        "    # Before dropping files\n",
        "    print(\"Before:\", obj.NCE_Labels_and_Files)\n",
        "\n",
        "    # Drop files 3 and 6 from NCE 0\n",
        "    obj.drop_files(**{'0': [3, 6]})\n",
        "\n",
        "    # After dropping files\n",
        "    print(\"After:\", obj.NCE_Labels_and_Files)\n",
        "    Expected Output:\n",
        "\n",
        "    This would be the expected output:\n",
        "\n",
        "    Before: {'0': [1, 2, 3, 4, 5, 6], '70': [10, 20, 30, 40], 90: [100, 200, 300]}\n",
        "    Updated NCE '0': [1, 2, 4, 5]\n",
        "    After: {'0': [1, 2, 4, 5], '70': [10, 20, 30, 40], '90': [100, 200, 300]}\n",
        "\n",
        "    Note: the outputted numbers would be the file names, so 1 would be something like data_001.csv\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "  def get_file_paths(self, reverse_order=False):\n",
        "    \"\"\"Associates each file number with a string of a file path from info_on_files_data\n",
        "    and returns a dictionary with file paths instead of file numbers. This function should be\n",
        "    used after NCE_Labels_and_Files is defined and drop_files is used if necessary.\n",
        "\n",
        "    Parameters:\n",
        "    reverse_order (bool): If True, the dictionary value is reversed to correctly map the file_paths.\n",
        "    It will correctly reverse the order back after mapping the corresponding file_paths.\n",
        "    Use this if the folder has the files from the last file at the top to the first file at the bottom.\n",
        "    \"\"\"\n",
        "    NCE_File_Paths = {}\n",
        "\n",
        "    # Gets the list of files from the directory\n",
        "    files = os.listdir(self.all_files_path)\n",
        "    # Sorts the files to ensure they are in the correct order\n",
        "    files.sort()  # Ensure files are sorted correctly\n",
        "\n",
        "    #print(\"Files in directory:\", files)  # Debug print to show the files\n",
        "\n",
        "    for NCE_value, file_names in self.NCE_Labels_and_Files.items():\n",
        "        file_paths = []\n",
        "        #print(f\"Processing NCE: {NCE_value} with files: {file_names}\")  # Debug print for current NCE and files\n",
        "\n",
        "        for file_name in file_names:  # Iterate through the actual file names\n",
        "            if file_name in files:  # Check if the file exists in the directory\n",
        "                file_path = os.path.join(self.all_files_path, file_name)\n",
        "                file_paths.append(file_path)\n",
        "            else:\n",
        "                print(f\"Notice: File '{file_name}' not found in the directory.\") if self.notices else None\n",
        "\n",
        "        if reverse_order:\n",
        "            # Reverses the order of the file paths if needed\n",
        "            file_paths.reverse()  # This restores the original order if needed\n",
        "\n",
        "        NCE_File_Paths[NCE_value] = file_paths  # Stores the list of file paths for each NCE value\n",
        "\n",
        "        print(f\"NCE File Paths for {NCE_value}: {file_paths}\") if self.print_information else None\n",
        "\n",
        "    #print(f\"Final NCE File Paths: {NCE_File_Paths}\")  # Debug print to show the final paths\n",
        "    self.NCE_File_Paths = NCE_File_Paths\n",
        "    return NCE_File_Paths\n",
        "\n",
        "  def merge__and_store_files_by_nce(self, nce_values=None, skip_rows = 7):\n",
        "        \"\"\"Merges all the file_paths from Mass_Spectrometer_Data.NCE_File_Paths and returns a dictionary containing the Pandas data frames.\n",
        "        Takes in NCE values in a list of the values you want to merge:\n",
        "\n",
        "        # Option 1: Merges files for all NCEs (default behavior)\n",
        "        merged_dataframes_all = the_method_over_all_files.merge_files_by_nce()\n",
        "\n",
        "        # Option 2: Merges files only for specified NCE values (e.g., NCE '0' and '70')\n",
        "        merged_dataframes_selected = the_method.merge_files_by_nce(nce_values=['0', '70'])\n",
        "\n",
        "        And you can access it by doing this:\n",
        "        Access merged DataFrame for NCE '0'\n",
        "        nce_0_df = NCE_dataframes_selected.get('0')\"\"\"\n",
        "\n",
        "        merged_dataframes = {}\n",
        "\n",
        "        # Ensure that self.Data has the NCE_File_Paths attribute\n",
        "        if not hasattr(self, 'NCE_File_Paths'):\n",
        "            raise AttributeError(\"self does not have an attribute 'NCE_File_Paths'.\") if self.errors else None\n",
        "\n",
        "        # If no NCE values are specified, use all keys from the NCE_File_Paths dictionary\n",
        "        if nce_values is None:\n",
        "            nce_values = list(self.NCE_File_Paths.keys())\n",
        "\n",
        "        # Loop through the specified NCE values and merge their corresponding files\n",
        "        for nce in nce_values:\n",
        "            if nce in self.NCE_File_Paths:  # Ensure NCE exists in the dictionary\n",
        "                df_list = []\n",
        "\n",
        "                for file_path in self.NCE_File_Paths[nce]:\n",
        "                  try:\n",
        "                      # Handling of bad lines\n",
        "                      df = pd.read_csv(file_path, on_bad_lines='skip', skiprows = skip_rows)  # Skip problematic lines.\n",
        "                      df_list.append(df)  # Add DataFrame to list\n",
        "                  except pd.errors.ParserError as e:\n",
        "                      print(f\"ParserError: Problem reading {file_path}: {e}\") if self.warnings else None\n",
        "                  except Exception as e:\n",
        "                      print(f\"Error reading {file_path}: {e}\") if self.warnings else None\n",
        "                # Concatenates all DataFrames in the list for the current NCE\n",
        "                if df_list:\n",
        "                    merged_dataframes[nce] = pd.concat(df_list, ignore_index=True)\n",
        "                else:\n",
        "                    print(f\"No valid data for NCE {nce}\") if self.warnings else None\n",
        "            else:\n",
        "                print(f\"NCE value '{nce}' not found in the NCE_File_Paths dictionary.\") if self.warnings else None\n",
        "\n",
        "        self.NCE_dataframes = merged_dataframes\n",
        "        return merged_dataframes\n",
        "\n",
        "class The_Method(Mass_Spectrometer_Data):\n",
        "  \"\"\"Draws comparisions among two different files of different NCE only\"\"\"\n",
        "  def __init__(self):\n",
        "    self.raw_data_1 = None\n",
        "    self.raw_data_2 = None\n",
        "    self.isotopes_1 = []\n",
        "    self.isotopes_2 = [] #You get Isotopes from the parent, but I included this in case it's desired.\n",
        "    self.masses_P = []\n",
        "    self.masses_D = []\n",
        "    self.areas_1 = []\n",
        "    self.areas_2 = []\n",
        "    self.ratios_1 = []\n",
        "    self.ratios_2 = []\n",
        "    self.alphas = []\n",
        "    self.enrichments = []\n",
        "    self.YN_list = []\n",
        "    self.preference_list = []\n",
        "    self.print_information = False\n",
        "\n",
        "  def copy_from(self, Mass_Spectrometer_Data_instance):\n",
        "        # Copy attributes from another Mass_Spectrometer_Data instance, assuming you want to use The_Method with the files from Mass_Spectrometer_Data.\n",
        "        self.molecule_name = Mass_Spectrometer_Data_instance.molecule_name\n",
        "        self.atom_name = Mass_Spectrometer_Data_instance.atom_name\n",
        "        self.date_data_was_taken = Mass_Spectrometer_Data_instance.date_data_was_taken\n",
        "        self.molecular_weight_of_molecule = Mass_Spectrometer_Data_instance.molecular_weight_of_molecule\n",
        "        self.AMU_Element = Mass_Spectrometer_Data_instance.AMU_Element\n",
        "        self.all_files_path = Mass_Spectrometer_Data_instance.all_files_path\n",
        "        self.info_on_files_path_or_URL = Mass_Spectrometer_Data_instance.info_on_files_path_or_URL\n",
        "        self.info_on_files_data = Mass_Spectrometer_Data_instance.info_on_files_data\n",
        "        self.file_names = Mass_Spectrometer_Data_instance.file_names\n",
        "        self.NCE = Mass_Spectrometer_Data_instance.NCE\n",
        "        self.NCE_Labels_and_File_Numbers = Mass_Spectrometer_Data_instance.NCE_Labels_and_Files\n",
        "        self.numeric_file_indices = Mass_Spectrometer_Data_instance.numeric_file_indices\n",
        "        self.NCE_File_Paths = Mass_Spectrometer_Data_instance.NCE_File_Paths\n",
        "        self.print_information = Mass_Spectrometer_Data_instance.print_information\n",
        "        self.errors = Mass_Spectrometer_Data_instance.errors\n",
        "        self.warnings = Mass_Spectrometer_Data_instance.warnings\n",
        "        self.notices = Mass_Spectrometer_Data_instance.notices\n",
        "        self.NCE_dataframes = Mass_Spectrometer_Data_instance.NCE_dataframes\n",
        "\n",
        "  def load_data(self, csv_file_1, csv_file_2, row_skip):\n",
        "   \"\"\"\"Load the data from the two CSV files you are going to compare into the Pandas Dataframe. Takes in two strings of the file_paths for the files and an integer for how many rows to skip. This is for the event that you don't\n",
        "   want to use the parent class Mass_Spectrometer_Data.\"\"\"\n",
        "   self.raw_data_1 = pd.read_csv(csv_file_1, skiprows = row_skip) # If the file has text other than the data, put the number of rows needed to skip to get to the data. Ensure all files are uniform\n",
        "   self.raw_data_2 = pd.read_csv(csv_file_2, skiprows = row_skip)\n",
        "\n",
        "  def calculate_areas_and_isotopes(self, row_start, row_end, bin_size, file_number, ROWSSKIPPED = 7, Calculate_Isotopes_and_Masses = False, rows_are_masses = False):\n",
        "    \"\"\" This will calculate areas and isotopes from the mass spectrometer data given the specifications. bin = int, row_start = int, row_end = int where the bin is a group of values you want to group together for each peak,\n",
        "    row_start and row_end are respectively where you'll start grouping data and where the last row will be. It is inclusive of the value you put in. Finally, you need to put which file you're calculating\n",
        "    the areas for, file = 1 or 2 for the respective files since the parameters of each is different. One note is that your rows should correspond to the row number in either excel or google sheets after deleting any rows before the headers and data. The indexing and reported bins values are specifically tailored for this.\n",
        "    Calculate Isotopes = False if you want the function to calculate areas only, set it to True if you need the isotope values. If you index by mass, ROWSSKIPPED is needed to specify how\n",
        "    many rows you'd skip in the original spreadsheet to get to the data and rows_are_mass must = True\"\"\"\n",
        "\n",
        "    # Select the correct DataFrame based on the file number\n",
        "    data = self.raw_data_1 if file_number == 1 else self.raw_data_2\n",
        "\n",
        "    # You can change these if you use something that didn't originate from google sheets or excel\n",
        "    pandas_difference_index = 1 # Pandas uses 0-Based indexing whereas google sheets uses 1-Based indexing, so the answer would be one index off. This accounts for that\n",
        "    header_affect_on_index = 1 # The index is also offset by the header, so this accounts for it being off by 1\n",
        "\n",
        "    if rows_are_masses:\n",
        "       masses_to_find = [row_start, row_end]\n",
        "\n",
        "       # Define tolerance for approximate matching due to floating decimals between files. Change if necessary for more precise matching\n",
        "       if \".\" in str(masses_to_find[0]):\n",
        "          decimal_part = str(masses_to_find[0]).split(\".\")[1]\n",
        "          if len(decimal_part) >= 5:\n",
        "            tolerance = 1e-5  # Match up to the fifth decimal place\n",
        "          else:\n",
        "            n = len(decimal_part)\n",
        "            tolerance = 10 ** -n  # To the nth decimal place\n",
        "       else:\n",
        "          tolerance = 1e-5  # Default tolerance if no decimal part exists\n",
        "\n",
        "       for m in masses_to_find:\n",
        "\n",
        "          print(f\"Mass {m}: Matches: {data['Mass'].apply(lambda x: abs(x - m) < tolerance).sum()}\")\n",
        "\n",
        "       # Find row indices where the \"Mass\" values are within the tolerance\n",
        "       row_indice = data[data['Mass'].apply(lambda x: any(abs(x - m) < tolerance for m in masses_to_find))].index.tolist()\n",
        "       row_indices = []\n",
        "       for index in row_indice:\n",
        "         row_indices.append(index + ROWSSKIPPED + header_affect_on_index + pandas_difference_index)\n",
        "       print(f\"rows_indicies: {row_indices}\") #Debugger\n",
        "       row_start = row_indices[0]\n",
        "       row_end = row_indices[1]\n",
        "       #Debugger in the event you want to see what the actual resulting indexes are. This would be needed if there are many decimals.\n",
        "       print(f\"row_start: {row_start}\")\n",
        "       print(f\"row_end: {row_end}\")\n",
        "\n",
        "    # Ensure row_start and row_end are within valid bounds\n",
        "    if row_start < 0 or row_end >= len(data) or row_start > row_end:\n",
        "        print(f\"Invalid row range: row_start={row_start}, row_end={row_end}, max_row={len(data)}\") if self.warnings else None\n",
        "        return []\n",
        "\n",
        "    areas = []  # List to hold area sums\n",
        "    isotopes, masses = [], [] if Calculate_Isotopes_and_Masses else None # Lists to hold isotope and masses values\n",
        "\n",
        "    # Loops through the DataFrame until row_start exceeds row_end\n",
        "    current_bin_start = row_start\n",
        "    while current_bin_start < row_end:\n",
        "        # To define the end of the current bin\n",
        "        current_bin_end = current_bin_start + bin_size\n",
        "\n",
        "        # To ensure the current bin does not exceed row_end\n",
        "        if current_bin_end > row_end :\n",
        "            print(f\"Warning: Adjusting bin size as it exceeds the specified row_end at current_bin_start={current_bin_start}.\") if self.warnings else None\n",
        "            current_bin_end = row_end + (current_bin_end - row_end) # Adjust to include the values that fit within the specified bin\n",
        "\n",
        "        # Select the current bin of rows\n",
        "        bin_data = data.iloc[current_bin_start- pandas_difference_index - header_affect_on_index:current_bin_end - pandas_difference_index -  header_affect_on_index  + 1]  # +1 to include current_bin_end\n",
        "\n",
        "        # Debugging statement\n",
        "        #print(f\"Current Bin: Start Row = {current_bin_start}, End Row = {current_bin_end}\")\n",
        "\n",
        "        # Ensure bin_data is not empty\n",
        "        if bin_data.empty:\n",
        "            print(\"Warning: bin_data is empty at current_bin_start:\", current_bin_start) if self.warnings else None\n",
        "            break\n",
        "\n",
        "        # Sum the Intensities for Area\n",
        "        bin_sum = bin_data['Intensity'].sum()\n",
        "        areas.append(bin_sum)\n",
        "\n",
        "        # Debugging statement for sum\n",
        "        #print(f\"Bin Intensity Sum = {bin_sum}\")\n",
        "\n",
        "        # Check if there are any intensities in the bin before proceeding\n",
        "        if not bin_data['Intensity'].empty:\n",
        "          if Calculate_Isotopes_and_Masses:\n",
        "            max_y_index = bin_data['Intensity'].idxmax()\n",
        "            corresponding_x_value = data['Mass'].iloc[max_y_index]  # Accessing using iloc\n",
        "            isotopes.append(round(self.AMU_Element - (self.molecular_weight_of_molecule - corresponding_x_value)))\n",
        "            masses.append(corresponding_x_value)\n",
        "            #print(max_y_index)  # For debugging, Get the index of the max intensity\n",
        "            #print(corresponding_x_value) # For debugging, see what x-value was found.\n",
        "        else:\n",
        "            print(\"Your data should not be missing values. Clean your data >:(\") if self.errors else None\n",
        "            break\n",
        "        # Increments current_bin_start to move to the next bin\n",
        "        current_bin_start += bin_size + 1 # Increment by the size of the bin\n",
        "\n",
        "    # Store areas and isotopes in class attributes\n",
        "    if file_number == 1:\n",
        "        self.areas_1 = areas\n",
        "        self.isotopes_1 = isotopes if Calculate_Isotopes_and_Masses else None\n",
        "        self.masses_P = masses if Calculate_Isotopes_and_Masses else None\n",
        "    else:\n",
        "        self.areas_2 = areas\n",
        "        self.isotopes_2 = isotopes if Calculate_Isotopes_and_Masses else None\n",
        "        self.masses_D = masses if Calculate_Isotopes_and_Masses else None\n",
        "\n",
        "    if self.print_information:\n",
        "      print(f\"Calculated Areas for file {file_number}: {areas}\")\n",
        "      print(f\"Calculated Isotopes values for file {file_number}: {isotopes}\") if Calculate_Isotopes_and_Masses else None\n",
        "      print(f\"Calculated Masses values for file {file_number}: {masses}\") if Calculate_Isotopes_and_Masses else None\n",
        "    return areas  # Returns the areas list, the isotope values are just stored.\n",
        "\n",
        "    # For reference, the molar mass of Nd(NO3)4 is 392.2616 g/mol.\n",
        "\n",
        "  def add_areas(self, row_start, row_end, bin_size, file_number, Calculate_Isotopes_and_Masses = False):\n",
        "    \"In some event where you have data files you need to add up but the peaks themselves are not in the same row index, you can use this function to use calculate_areas_and_isotopes in the exact same manner, except for different row start and row end\"\n",
        "    Areas = np.array(self.areas_1 if file_number == 1 else self.areas_2)\n",
        "    self.calculate_areas_and_isotopes(row_start, row_end, bin_size, file_number, Calculate_Isotopes_and_Masses)\n",
        "    New_areas = np.array(self.areas_1 if file_number == 1 else self.areas_2)\n",
        "    Calculate_New_Areas = Areas + New_areas\n",
        "    if file_number == 1:\n",
        "      self.areas_1 = Calculate_New_Areas.tolist()\n",
        "    else:\n",
        "      self.areas_2 = Calculate_New_Areas.tolist()\n",
        "    print(f\"Added Areas: {Calculate_New_Areas.tolist()}\") if self.print_information else None\n",
        "\n",
        "    self.calculate_areas_and_isotopes(row_start, row_end, bin_size, file_number, Calculate_Isotopes_and_Masses)\n",
        "\n",
        "\n",
        "  def calculate_ratio(self, Normalization_Number, *areas, file_number):\n",
        "      \"\"\"Calculates either the ratio of either a list of areas, several individual areas, or one individual area as inputted from *areas. The number will either assign the results to ratios_1 or ratios_2,\n",
        "      Normalization_Number is an integer of the number you'll be using to normalize the data for the ratios as an integer value and file_number is also an integer value as to which file you want to calculate the ratios for.\"\"\"\n",
        "      if file_number != 1 and file_number != 2:\n",
        "        print(f\"An error will occur, you need to make the input 'number' either 1 or 2 to store the information in self.ratios_1 or self.ratios_2\") if self.warnings else None\n",
        "      else:\n",
        "        ratio_lst = []\n",
        "        if len(areas) == 1 and isinstance(areas[0], list):\n",
        "            for area in areas[0]: #loops over the list inside areas[0] which is needed since the list in *args will become a tuple\n",
        "              rat = area/Normalization_Number\n",
        "              ratio_lst.append(rat) # Append each ratio between two peaks to the list\n",
        "            if file_number == 1:\n",
        "              self.ratios_1 = ratio_lst\n",
        "            else:\n",
        "              self.ratios_2 = ratio_lst\n",
        "        else:\n",
        "          for area in areas:\n",
        "            rat = area / Normalization_Number\n",
        "            ratio_lst.append(rat)\n",
        "          if file_number == 1:\n",
        "              self.ratios_1 = ratio_lst\n",
        "          else:\n",
        "              self.ratios_2 = ratio_lst\n",
        "        print(f\"Calculated Ratios for file {file_number}: {ratio_lst}\") if self.print_information else None\n",
        "        return ratio_lst\n",
        "\n",
        "  \"\"\" Test cases for clarity as to how this function works:\n",
        "    1.) For a list of areas: print(calculate_ratio(10, [20, 30, 40])) -> [2.0, 3.0, 4.0]\n",
        "    2.) for Individual areas: print(calculate_ratio(10, 20, 30, 40)) -> [2.0, 3.0, 4.0]\n",
        "    3.) For a single area: print(calculate_ratio(10, 20)) -> [2.0] which is the singular ratio\"\"\"\n",
        "\n",
        "  def calculate_alpha(self, ratio_1, ratio_2):\n",
        "      \"\"\"Calculate the alpha of a specific ratio between the two files calculated ratios\"\"\"\n",
        "      return ratio_2/ratio_1 if ratio_1 and ratio_2 is not None else None\n",
        "\n",
        "  def make_alphas_list(self):\n",
        "      if len(self.ratios_1) != len(self.ratios_2):\n",
        "        print(f\"Type Error: The amount of ratios from the two files are not equal, you should use calculate alpha to manually assign the peaks you want to compare\") if self.warnings else None\n",
        "      alphas_lst = []\n",
        "      list_length_differences = max(len(self.ratios_1), len(self.ratios_2)) - min(len(self.ratios_1), len(self.ratios_2))\n",
        "      for i in range(min(len(self.ratios_1), len(self.ratios_2))):\n",
        "        current_alpha = self.ratios_2[i]/self.ratios_1[i]\n",
        "        alphas_lst.append(current_alpha)\n",
        "      if list_length_differences != 0:\n",
        "        while list_length_differences != 0: #This ensures it won't error even with unequal list sizes.\n",
        "          list_length_differences = list_length_differences - 1\n",
        "          alphas_lst.append(None)\n",
        "      self.alphas = alphas_lst\n",
        "      print(f\"Calculated Alphas: {alphas_lst}\") if self.print_information else None\n",
        "      return self.alphas\n",
        "\n",
        "  def calculate_enrichment(self, alpha):\n",
        "        return 1- alpha\n",
        "\n",
        "  def enrichment_list(self):\n",
        "      enrichment_list = [self.calculate_enrichment(alpha) for alpha in self.alphas]\n",
        "      self.enrichments = enrichment_list\n",
        "      print(f\"Calculated Enrichment: {enrichment_list}\") if self.print_information else None\n",
        "      return enrichment_list\n",
        "\n",
        "  def isotope_effect(self):\n",
        "      YN_lst = self.YN_list\n",
        "      for alpha in self.alphas:\n",
        "        if alpha == 1:\n",
        "          YN_lst.append('No')\n",
        "        else:\n",
        "          YN_lst.append('Yes')\n",
        "      self.YN_list = YN_lst\n",
        "      print(f\"Calculated YN: {YN_lst}\") if self.print_information else None\n",
        "      return YN_lst\n",
        "\n",
        "  def preferences(self):\n",
        "      preference_lst = self.preference_list\n",
        "      for enrichment in self.enrichments:\n",
        "        if enrichment > 1:\n",
        "          preference_lst.append(\"D\") #preference for daughter species\n",
        "        elif enrichment < 1:\n",
        "          preference_lst.append(\"P\") #preference for parent species\n",
        "        else:\n",
        "          preference_lst.append(\"=\") #equal preference\n",
        "      self.preference_list = preference_lst\n",
        "      print(f\"Calculated Preferences: {preference_lst}\") if self.print_information else None\n",
        "      return self.preference_list\n",
        "\n",
        "  def drop(self, edit = False, *what_indexes, **what_list):\n",
        "      \"\"\"Drop specific values from the self.class attributes.\n",
        "\n",
        "      Args:\n",
        "          what_indexes (int): Indexes to drop from the list.\n",
        "          edit (bool): If True, modifies the actual attribute by removing values.\n",
        "          what_list (str): The name of the attribute to drop from.\n",
        "      \"\"\"\n",
        "      for list_name, _ in what_list.items():\n",
        "          if hasattr(self, list_name):\n",
        "              target_list = getattr(self, list_name)\n",
        "              if edit:\n",
        "                  for i in sorted(what_indexes, reverse=True):  # Sort to avoid index shift\n",
        "                      if i < len(target_list):\n",
        "                          target_list.pop(i)\n",
        "              else:\n",
        "                  print([target_list[i] for i in what_indexes if i < len(target_list)]) if self.print_information else None\n",
        "          else:\n",
        "            print(f\"Attribute '{list_name}' not found in the class.\") if self.notices else None\n",
        "\n",
        "# Error propagation\n",
        "class Error_Propagation(Mass_Spectrometer_Data):\n",
        "    def __init__(self, Mass_Spectrometer_Data_Instance):\n",
        "        self.Data = Mass_Spectrometer_Data_Instance\n",
        "        self.The_Method_Function_Access = The_Method()\n",
        "        self.The_Method_Function_Access.copy_from(self.Data)\n",
        "        self.parent_total_areas, self.daughter_total_areas = [], []\n",
        "        self.Error_D, self.Error_P = [], []\n",
        "        self.δ_PA, self.δ_DA = [],[]\n",
        "        self.alphas = []\n",
        "        self.enrichments = []\n",
        "        self.δα = []\n",
        "        self.Isotopes = []\n",
        "        self.masses_P = []\n",
        "        self.masses_D = []\n",
        "        self.Parent_ratios = []\n",
        "        self.Daughter_ratios = []\n",
        "        self.print_information = False\n",
        "        self.file_names_P = []\n",
        "        self.file_names_D = []\n",
        "        self.Preference_list = []\n",
        "\n",
        "    def Individual_δ(self, *Areas):\n",
        "      \"\"\"Calculates the individual δ of every Area in the list or individual area depending what is passed in. This allows you to individually select Areas to find the error of.\"\"\"\n",
        "      if len(Areas) == 1 and isinstance(Areas[0], list):\n",
        "        return np.sqrt(np.array(Areas[0])).tolist()\n",
        "      else:\n",
        "        return math.sqrt(Areas[0])\n",
        "\n",
        "    def calculate_δα_indv(self, row_starts_and_ends_parent, row_starts_and_ends_daughter, Bin_size, number_of_peaks, normalization, parent_daughter, ROWSKIP = 7, NBTA = False, rows_are_masses = False):\n",
        "      \"\"\"Calculates δα for each individual area as a method of getting error bars. Then combines them. This method allows you to keep track of each individual error before the Areas are added up and keep track of all data being\n",
        "      produced up to the calculation of the errors.\n",
        "\n",
        "      Specifications:\n",
        "      row_starts_and_ends_parent = [row_start_parent, row_end_parent] if you have evenly spaced peaks, otherwise you can put [[row_start_parent_1, row_end_parent_1], [row_start_parent_2, row_end_parent_2]] and so on for any unevenly spaced peaks.\n",
        "      row_starts_and_ends_daughter = same as row_starts_and_ends_parent, but for daughters\n",
        "      Bin_Size = integer of how many values you want for each rows in the spreedsheet\n",
        "      number_of_peaks = integer of how many peaks corresponding to the lighter molecules you found.\n",
        "      normalization = integer corresponding to number you want to use to normalize the data to.\n",
        "      parent_daughter = = ['parent_NCE_Value', 'daughter_NCE_Value'] to compare between two NCE (of consistent isolations) data.\n",
        "      NBTA is True if you want to normalize by the total areas, otherwise it'll use the normalization integer you put.\n",
        "\n",
        "      \"\"\"\n",
        "\n",
        "      # So that the function knows these variables refer to the global variables when it is used in an if else statement to check that they're empty since they will be defined only once\n",
        "      global All_Isotopes\n",
        "      global Parent_masses\n",
        "      global Daughter_masses\n",
        "      global Atom_Names\n",
        "\n",
        "      # Uses the specified NCE values as keys\n",
        "      parent = parent_daughter[0]\n",
        "      daughter = parent_daughter[1]\n",
        "\n",
        "      # Gathers the file paths for the specified keys. Checks if the specified parent_daughter pair works, if not it'll attempt to use the file_paths that would be gathered from seperate_files.\n",
        "      Parent_Files = self.Data.NCE_File_Paths[parent]\n",
        "      Daughter_Files = self.Data.NCE_File_Paths[daughter]\n",
        "\n",
        "      # Create all variables to store information in\n",
        "      parent_areas, parent_error, daughter_areas, daughter_error, parent_percent_error, daughter_percent_error, δ_PA, δ_DA, δα= ([] for _ in range(9))\n",
        "      xx, yy = np.zeros(number_of_peaks), np.zeros(number_of_peaks) # Total parent Areas array, Total daughter Areas array\n",
        "\n",
        "      # To count through the file names for labeling purposes\n",
        "      file_tracker_D = -1\n",
        "      file_tracker_P = -1\n",
        "      file_number_P, file_number_D = self.Data.numeric_file_indices[:len(Parent_Files)], self.Data.numeric_file_indices[len(Parent_Files):] #Actual number of file\n",
        "      self.file_names_P = file_number_P\n",
        "      self.file_names_D = file_number_D\n",
        "\n",
        "      #Summing areas of the same peak areas across all specified files\n",
        "      for file_P in Parent_Files:\n",
        "        file_tracker_P += 1\n",
        "        self.The_Method_Function_Access.raw_data_1 = pd.read_csv(file_P, skiprows = ROWSKIP)\n",
        "        x = np.array([])\n",
        "        for row_starts_and_ends in row_starts_and_ends_parent:\n",
        "          start_P, end_P = row_starts_and_ends[0], row_starts_and_ends[1]\n",
        "          if rows_are_masses:\n",
        "            area_list = self.The_Method_Function_Access.calculate_areas_and_isotopes(start_P, end_P, Bin_size, 1, ROWSSKIPPED = ROWSKIP, rows_are_masses = True)\n",
        "          else:\n",
        "            area_list = self.The_Method_Function_Access.calculate_areas_and_isotopes(start_P - ROWSKIP, end_P - ROWSKIP, Bin_size, 1, ROWSSKIPPED = ROWSKIP)\n",
        "          x = np.concatenate([x, area_list])\n",
        "        print(f\"Areas for P file {file_number_P[file_tracker_P]}: {x}\") if self.print_information else None\n",
        "        print(f\" δ_PA file {file_number_P[file_tracker_P]}: {np.sqrt(x)}\") if self.print_information else None\n",
        "        xx = xx + x\n",
        "      for file_D in Daughter_Files:\n",
        "        file_tracker_D += 1\n",
        "        self.The_Method_Function_Access.raw_data_2 = pd.read_csv(file_D, skiprows = ROWSKIP)\n",
        "        y = np.array([])\n",
        "        for row_starts_and_end in row_starts_and_ends_daughter:\n",
        "            start, end = row_starts_and_end[0], row_starts_and_end[1]\n",
        "            if rows_are_masses:\n",
        "              area_lst = self.The_Method_Function_Access.calculate_areas_and_isotopes(start, end, Bin_size, 2, ROWSSKIPPED = ROWSKIP, rows_are_masses = True)\n",
        "            else:\n",
        "              area_lst = self.The_Method_Function_Access.calculate_areas_and_isotopes(start - ROWSKIP, end - ROWSKIP, Bin_size, 2, ROWSSKIPPED = ROWSKIP)\n",
        "            y = np.concatenate([y, area_lst])\n",
        "        print(f\"Areas for D file {file_number_D[file_tracker_D]}: {y}\") if self.print_information else None\n",
        "        print(f\" δ_DA file {file_number_D[file_tracker_D]}: {np.sqrt(y)}\") if self.print_information else None\n",
        "        yy = yy + y\n",
        "      self.parent_total_areas = xx\n",
        "      if self.Data.date_data_was_taken in All_Parent_Areas:\n",
        "        key = self.Data.date_data_was_taken\n",
        "        new_key = key\n",
        "        # Looping to check if the key exists and to keep appending a prime until it's a unique key\n",
        "        while new_key in All_Parent_Areas:\n",
        "          new_key = new_key + \"'\"\n",
        "        All_Parent_Areas[new_key] = tuple(xx)\n",
        "      else:\n",
        "        All_Parent_Areas[self.Data.date_data_was_taken] = tuple(xx)\n",
        "      self.daughter_total_areas = yy\n",
        "      if self.Data.date_data_was_taken in All_Daughter_Areas:\n",
        "        key = self.Data.date_data_was_taken\n",
        "        new_key = key\n",
        "        # Looping to check if the key exists and to keep appending a prime until it's a unique key\n",
        "        while new_key in All_Daughter_Areas:\n",
        "          new_key = new_key + \"'\"\n",
        "        All_Daughter_Areas[new_key] = tuple(yy)\n",
        "      else:\n",
        "        All_Daughter_Areas[self.Data.date_data_was_taken] = tuple(yy)\n",
        "      print(f\"total areas for P: {xx}\") if self.print_information else None\n",
        "      print(f\"total areas for D: {yy}\") if self.print_information else None\n",
        "      δ_PA = np.sqrt(xx)\n",
        "      δ_DA = np.sqrt(yy)\n",
        "      self.δ_PA = δ_PA\n",
        "      self.δ_DA = δ_DA\n",
        "      print(f\"δ_PA: {δ_PA}\") if self.print_information else None\n",
        "      print(f\"δ_DA: {δ_DA}\") if self.print_information else None\n",
        "      Error_P = δ_PA/xx\n",
        "      Error_D= δ_DA/yy\n",
        "      print(f\"% error for P: {Error_P}\") if self.print_information else None\n",
        "      print(f\"% error for D: {Error_D}\") if self.print_information else None\n",
        "      self.Error_D = Error_D\n",
        "      self.Error_P = Error_P\n",
        "      daughter_ratios = yy/np.sum(yy) if NBTA else yy/normalization\n",
        "      parent_ratios = xx/np.sum(xx) if NBTA else xx/normalization\n",
        "      print(f\"Parent Ratios: {parent_ratios}\") if self.print_information else None\n",
        "      print(f\"Daughter Ratios: {daughter_ratios}\") if self.print_information else None\n",
        "      self.Parent_ratios = parent_ratios\n",
        "      self.Daughter_ratios = daughter_ratios\n",
        "\n",
        "      alpha = daughter_ratios/parent_ratios\n",
        "      self.alphas = alpha\n",
        "      print(f\"Alpha Values: {alpha}\") if self.print_information else None\n",
        "      if self.Data.date_data_was_taken in All_alphas:\n",
        "        key = self.Data.date_data_was_taken\n",
        "        new_key = key\n",
        "        while new_key in All_alphas:\n",
        "          new_key = new_key + \"'\"\n",
        "        All_alphas[new_key] = tuple(alpha)\n",
        "      else:\n",
        "        All_alphas[self.Data.date_data_was_taken] = tuple(alpha) # Puts the alpha values in the global variable that is a dictionary of all alphas corresponding to a key which is the date the data was taken.\n",
        "\n",
        "      Enrichment = alpha - 1\n",
        "      self.enrichments = Enrichment\n",
        "      print(f\"Enrichment Values: {Enrichment}\") if self.print_information else None\n",
        "      if self.Data.date_data_was_taken in All_enrichments:\n",
        "        key = self.Data.date_data_was_taken\n",
        "        new_key = key\n",
        "        while new_key in All_enrichments:\n",
        "          new_key = new_key + \"'\"\n",
        "        All_enrichments[new_key] = tuple(Enrichment)\n",
        "      else:\n",
        "        All_enrichments[self.Data.date_data_was_taken] = tuple(Enrichment) # Puts the alpha values in the global variable that is a dictionary of all alphas corresponding to a key which is the date the data was taken.\n",
        "\n",
        "      δα = (alpha * np.sqrt((Error_D) ** 2 + (Error_P) ** 2))\n",
        "      self.δα = δα\n",
        "      print(f\"δα: {δα}\") if self.print_information else None\n",
        "      if self.Data.date_data_was_taken in All_Errors:\n",
        "        key = self.Data.date_data_was_taken\n",
        "        new_key = key\n",
        "        while new_key in All_Errors:\n",
        "          new_key = new_key + \"'\"\n",
        "        All_Errors[new_key] = tuple(δα)\n",
        "      else:\n",
        "        All_Errors[self.Data.date_data_was_taken] = tuple(δα) # Puts the Error values in the global variable that is a dictionary of all Errors corresponding to a key which is the date the data was taken.\n",
        "\n",
        "      #Getting the isotopes and masses\n",
        "      self.The_Method_Function_Access.raw_data_1 = pd.read_csv(Parent_Files[0], skiprows = ROWSKIP)\n",
        "      ISOTOPES = []\n",
        "      PARENT_MASSES = []\n",
        "      DAUGHTER_MASSES = []\n",
        "      self.The_Method_Function_Access.raw_data_1 = pd.read_csv(Parent_Files[0], skiprows = ROWSKIP)\n",
        "      self.The_Method_Function_Access.raw_data_2 = pd.read_csv(Daughter_Files[0], skiprows = ROWSKIP)\n",
        "      for row_starts_and_ends in row_starts_and_ends_parent:\n",
        "        start_P, end_P = row_starts_and_ends[0], row_starts_and_ends[1]\n",
        "        if rows_are_masses:\n",
        "          self.The_Method_Function_Access.calculate_areas_and_isotopes(start_P, end_P, Bin_size, 1, ROWSSKIPPED = ROWSKIP, Calculate_Isotopes_and_Masses = True, rows_are_masses = True)\n",
        "        else:\n",
        "          self.The_Method_Function_Access.calculate_areas_and_isotopes(start_P - ROWSKIP, end_P - ROWSKIP, Bin_size, 1, ROWSSKIPPED = ROWSKIP, Calculate_Isotopes_and_Masses = True)\n",
        "        for i in self.The_Method_Function_Access.isotopes_1:\n",
        "          ISOTOPES.append(i)\n",
        "        for j in self.The_Method_Function_Access.masses_P:\n",
        "          PARENT_MASSES.append(j)\n",
        "      print(f\"Isotopes: {ISOTOPES}\") if self.print_information else None\n",
        "      if All_Isotopes == (): # This is to make the All_Isotopes global variable have the isotopes. This only occurs once when the tuple is empty.\n",
        "        All_Isotopes = tuple(ISOTOPES)\n",
        "      print(f\"Masses Parent: {PARENT_MASSES}\") if self.print_information else None\n",
        "      for row_starts_and_ends in row_starts_and_ends_daughter:\n",
        "        start_D, end_D = row_starts_and_ends[0], row_starts_and_ends[1]\n",
        "        if rows_are_masses:\n",
        "          self.The_Method_Function_Access.calculate_areas_and_isotopes(start_D, end_D, Bin_size, 2, ROWSSKIPPED = ROWSKIP, Calculate_Isotopes_and_Masses= True, rows_are_masses = True)\n",
        "        else:\n",
        "          self.The_Method_Function_Access.calculate_areas_and_isotopes(start_D - ROWSKIP, end_D - ROWSKIP, Bin_size, 2, ROWSSKIPPED = ROWSKIP, Calculate_Isotopes_and_Masses= True)\n",
        "        for k in self.The_Method_Function_Access.masses_D:\n",
        "          DAUGHTER_MASSES.append(k)\n",
        "      print(f\"Masses Daughter: {DAUGHTER_MASSES}\") if self.print_information else None\n",
        "      self.Isotopes = ISOTOPES\n",
        "      self.masses_P = PARENT_MASSES\n",
        "      self.masses_D = DAUGHTER_MASSES\n",
        "      #For storing parent and daughter masses in the Parent_masses and Daughter_Masses global variables only once when they are empty\n",
        "      if Parent_masses == ():\n",
        "        Parent_Masses = tuple(PARENT_MASSES)\n",
        "      if Daughter_masses == ():\n",
        "        Daughter_Masses = tuple(DAUGHTER_MASSES)\n",
        "\n",
        "      if Atom_Names == \" \":\n",
        "        Atom_Names = self.The_Method_Function_Access.atom_name\n",
        "\n",
        "      self.Preference_list = np.array(['Parent' if ε < 0 else 'Neither' if ε == 0 else 'Daughter' for ε in self.enrichments])\n",
        "      # If you notice any data you want to remove, use the drop_files function in the parent class Mass_Spectrometer_Data, run the get_file_paths function again, then perform all calculations\n",
        "\n",
        "# Visualizing the data\n",
        "class Graphs():\n",
        "  \"\"\"Used to generate all types of Graphs for the data between two specific values after all the data has been produced in the other Classes. There are graphs that are in the global function section.\n",
        "  The save_pdf in each graph is used to not do plt.show() so that it can be saved as a figure instead in the Make_PDF class (True to make_pdfs, False to plot the graph).\"\"\"\n",
        "  def __init__(self, Mass_Spectrometer_Data_Instance, Error_instance):\n",
        "    self.MSDI = Mass_Spectrometer_Data_Instance #Specify for which graphs you want to create.\n",
        "    self.error = Error_instance  #Specify Errors for which graphs you'd want to create\n",
        "    self.professional = False\n",
        "\n",
        "  def IVM(self, file_path, xlim_left, xlim_right, plot_title, save_pdf = False):\n",
        "    \"\"\"Generates a Intensity Vs Mass graph from the data\n",
        "    Enter how many rows you need to skip to access the data into row_skip as an integer\n",
        "    Enter where you want the graphs to start and end to zoom in as integers, into xlim_left and xlim_right respectively\n",
        "    Enter the plot_title as a string of what you want to name the graph into plot_title\"\"\"\n",
        "    data = pd.read_csv(file_path, skiprows = 7)\n",
        "    Column_labels = ['Mass', 'Intensity']\n",
        "    data.columns = Column_labels\n",
        "    x_axis = 'Mass'\n",
        "    y_axis = 'Intensity'\n",
        "    plt.plot(data[x_axis], data[y_axis], label = f'{y_axis} vs {x_axis}')\n",
        "    plt.xlabel(x_axis)\n",
        "    plt.ylabel(y_axis)\n",
        "    plt.title(plot_title)\n",
        "    plt.xlim(xlim_left, xlim_right)\n",
        "    if not save_pdf:\n",
        "       plt.show()\n",
        "\n",
        "  def generate_all_IVM(self, xlim_left_P, xlim_right_P, xlim_left_D, xlim_right_D, Parent_Daughter, save_pdf = False):\n",
        "      \"\"\"Generates all Intensity versus Masses graphs for every file for the parent and daughter species. I assume you only have two for this\"\"\"\n",
        "      Parent_key = Parent_Daughter[0] #To get the file_paths for the specifc NCE values in NCE_File_Paths\n",
        "      Daughter_key = Parent_Daughter[1]\n",
        "      P = 0 #Initalize counters to keep number the files in order\n",
        "      D = 0\n",
        "      for i in self.MSDI.NCE_File_Paths[Parent_key]:\n",
        "        plot_title_P = \"Intensity Vs Mass\" + \" \" + \"For Parent File:\" + \" \" + str(P) + \" \" + \"on\" + \" \" + self.MSDI.date_data_was_taken\n",
        "        self.IVM(i, xlim_left_P, xlim_right_P, plot_title_P) if not save_pdf else self.IVM(i, xlim_left_P, xlim_right_P, plot_title_P, save_pdf = True)\n",
        "        P += 1\n",
        "      for j in self.MSDI.NCE_File_Paths[Daughter_key]:\n",
        "        plot_title_D = \"Intensity Vs Mass\" + \" \" + \"for Daughter File:\" + \" \" + str(D) + \" \" + \"on\" + \" \" + self.MSDI.date_data_was_taken\n",
        "        self.IVM(j, xlim_left_D, xlim_right_D, plot_title_D) if not save_pdf else self.IVM(j, xlim_left_D, xlim_right_D, plot_title_D, save_pdf = True)\n",
        "        D += 1\n",
        "\n",
        "  def Big_IVM(self, Parent_Daughter, save_pdf = False):\n",
        "      \"\"\"IVM showing the total combined data frames from all the files for each NCE Parent_Daughter is list of strings for NCE values you are representing as parent and daughter such as ['0','70']\"\"\"\n",
        "      Parent_key = Parent_Daughter[0]\n",
        "      Daughter_key = Parent_Daughter[1]\n",
        "      Parent =  self.MSDI.NCE_dataframes[Parent_key]\n",
        "      Daughter = self.MSDI.NCE_dataframes[Daughter_key]\n",
        "      Column_labels = ['Mass', 'Intensity']\n",
        "      Parent.columns = Column_labels\n",
        "      Daughter.columns = Column_labels\n",
        "      x_axis = 'Mass'\n",
        "      y_axis = 'Intensity'\n",
        "      plt.plot(Parent[x_axis], Parent[y_axis], label = f'{y_axis} vs {x_axis}')\n",
        "      plt.xlabel(x_axis)\n",
        "      plt.ylabel(y_axis)\n",
        "      plt.title(\"Fragmentation Analysis: Intensity Patterns Across Parent Molecule Masses\" + self.MSDI.date_data_was_taken)\n",
        "      plt.show()\n",
        "      plt.plot(Daughter[x_axis], Daughter[y_axis], label = f'{y_axis} vs {x_axis}')\n",
        "      plt.xlabel(x_axis)\n",
        "      plt.ylabel(y_axis)\n",
        "      plot_title_D = \"Fragmentation Analysis: Intensity Patterns Across Daughter Molecule Masses\" + self.MSDI.date_data_was_taken\n",
        "      plt.title(plot_title_D)\n",
        "      if not save_pdf:\n",
        "       plt.show()\n",
        "\n",
        "  def εVIs(self, save_pdf = False):\n",
        "    \"\"\"Generates an Basic Enrichment vs Isotope Graph with error bars from data calculated in a method. Make the plot title a string of the title you want.\"\"\"\n",
        "    Column_labels = ['Isotope', 'ε']\n",
        "    x_axis = self.error.Isotopes\n",
        "    y_axis = self.error.enrichments\n",
        "    yerr = self.error.δα\n",
        "    custom_labels = []\n",
        "    for isotope in self.error.Isotopes:\n",
        "      isotope_label = f\"$\\\\mathrm{{^{{{isotope}}}{self.MSDI.atom_name}}}$\"\n",
        "      custom_labels.append(isotope_label)  # Custom labels\n",
        "    plt.plot(x_axis, y_axis, color = '#1a2b6d') if self.professional else plt.plot(x_axis, y_axis, color = (0.6, 0.8, 1.0))\n",
        "    plt.xticks(x_axis, custom_labels)\n",
        "    plt.xlabel('Isotope')\n",
        "    plt.ylabel('ε')\n",
        "    plt.title((\"Exploring the Relationship Between Enrichment Levels and Isotopic Composition\" + \" \" + self.MSDI.date_data_was_taken))\n",
        "    plt.errorbar(x_axis, y_axis, yerr=yerr, fmt='s', color ='#8b0000', ecolor= '#888888', capsize=15) if self.professional else plt.errorbar(x_axis, y_axis, yerr=yerr, fmt='s', color = (1.0,0.5,0.4), ecolor= (0.5,0.0,0.5), capsize=15) # A soft serenity color theme\n",
        "    graph_name = \"Exploring the Relationship Between Enrichment Levels and Isotopic Composition\" + \" \" + self.MSDI.date_data_was_taken\n",
        "    for i, txt in enumerate(self.error.enrichments.tolist()):\n",
        "        plt.annotate(txt, (x_axis[i], y_axis[i]))\n",
        "    if not save_pdf:\n",
        "       plt.show()\n",
        "\n",
        "  def αVIs(self, save_pdf = False):\n",
        "    \"\"\"Generates a Alpha Vs Isotope graph from the data\"\"\"\n",
        "    # Purely for Decoration\n",
        "    if not self.professional:\n",
        "      Color_Themes_Picker = np.array([0,1,2,3])\n",
        "      Random_Theme = np.random.choice(Color_Themes_Picker)\n",
        "      Line_Colors = [(0.0,0.2,0.8), '#6dbb22', '#d52d00', '#3b9d1e' ]\n",
        "      Dot_Colors = [(0.7,0.5,0.9), '#8c5c2f', '#0033cc', '#2e8b57' ]\n",
        "      Error_Bar_colors = ['green',  '#4a7023', '#003399', '#001f4d' ]\n",
        "    Column_labels = ['Isotope', 'α']\n",
        "    x_axis = self.error.Isotopes\n",
        "    custom_labels = []\n",
        "    for isotope in self.error.Isotopes:\n",
        "      isotope_label = f\"$\\\\mathrm{{^{{{isotope}}}{self.MSDI.atom_name}}}$\"\n",
        "      custom_labels.append(isotope_label)  # Custom labels\n",
        "    y_axis = self.error.alphas\n",
        "    yerr = self.error.δα\n",
        "    plt.plot(x_axis, y_axis, color = '#1a2b6d') if self.professional else plt.plot(x_axis, y_axis, color = Line_Colors[Random_Theme])\n",
        "    plt.xticks(x_axis, custom_labels)\n",
        "    plt.xlabel('Isotope')\n",
        "    plt.ylabel('α')\n",
        "    plt.title(\"Alpha Coefficient Trends as a Function of Isotopic Composition\" + \" \" + self.MSDI.date_data_was_taken)\n",
        "    plt.errorbar(x_axis, y_axis, yerr=yerr, fmt='s', color ='#8b0000', ecolor= '#888888', capsize=15) if self.professional else plt.errorbar(x_axis, y_axis, yerr=yerr, fmt='s', color = Dot_Colors[Random_Theme], ecolor = Error_Bar_colors[Random_Theme], capsize=15)\n",
        "    graph_name = \"Alpha Coefficient Trends as a Function of Isotopic Composition\" + \" \" + self.MSDI.date_data_was_taken\n",
        "    for i, txt in enumerate(self.error.alphas.tolist()):\n",
        "        plt.annotate(txt, (x_axis[i], y_axis[i]))\n",
        "    if not save_pdf:\n",
        "       plt.show()\n",
        "\n",
        "\n",
        "  def RIVM(self, PDF_Maker = None):\n",
        "    \"\"\"Relative intensity vs masses for parent and daughter graphs\"\"\"\n",
        "    # Purely for Decoration\n",
        "    if not self.professional:\n",
        "      Color_Themes_Picker_1 = np.array([0,1,2,3,4,5,6,7])\n",
        "      Color_Themes_Picker_2 = np.array([0,1,2,3,4,5,6,7])\n",
        "      Random_Theme_1 = np.random.choice(Color_Themes_Picker_1)\n",
        "      Random_Theme_2 = np.random.choice(Color_Themes_Picker_2)\n",
        "      Line_Colors_1 = [(0.7,0.1,0.1), '#a23b3b', '#d16002', '#ff6f61', '#FFD700', \"#8C1515\", \"#FFA500\", \"#D4A017\"]\n",
        "      Dot_Colors_1 = [(1.0, 0.85, 0.0), '#d9a673', '#9e2a2b','#f28a30', '#FF6F61', \"#B3995D\",\"#FF69B4\", \"#3366CC\"]\n",
        "      Error_Bar_colors_1 = [(1.0, 0.6, 0.2), '#ffb087', '#b17a34', '#f7c6a3', '#FFB6A0', \"#4D4F53\",\"#FFD700\", \"#FFCC33\"]\n",
        "      Line_Colors_2 = [(0.2,0.4,0.2),  '#2a4d69', '#006d77', '#191970', '#2e5d55', '#0f4c81', \"#003262\", \"#0099FF\", \"#87CEEB\"]\n",
        "      Dot_Colors_2 = [(0.5,0.25,0.1),  '#81894e', '#ff6b6b', '#7a7bb0', '#4682b4', '#84c0c6', \"#FDB515\", \"#CC6600\", \"#8B4513\"]\n",
        "      Error_Bar_colors_2 = [(0.6,1.0,0.6), '#4b8e8d', '#83c5be', '#a4c3e2', '#b0c4de', '#d8e2dc', \"#BDC3C7\", \"#66BB66\", \"#228B22\"]\n",
        "    #Actual plotting\n",
        "    plt.figure(figsize = (10,5))\n",
        "    Column_labels = ['Mass', 'Relative Intensity']\n",
        "    x_axis = self.error.masses_P #rounded\n",
        "    y_axis = self.error.parent_total_areas.tolist()\n",
        "    yerr = self.error.Error_P\n",
        "    plt.plot(x_axis, y_axis, color = '#1a2b6d') if self.professional else plt.plot(x_axis, y_axis, color = Line_Colors_1[Random_Theme_1])\n",
        "    ratio_label_P = self.error.Parent_ratios.tolist()\n",
        "    plt.xlabel('Mass')\n",
        "    plt.ylabel('Relative Intensity')\n",
        "    plt.title(\"Relative Intensity by Mass in Parent Molecule Fragmentation\" + \" \" + self.MSDI.date_data_was_taken)\n",
        "    plt.errorbar(x_axis, y_axis, yerr=yerr, fmt='s', color ='#8b0000', ecolor= '#888888', capsize=15) if self.professional else plt.errorbar(x_axis, y_axis, yerr=yerr, fmt='s', color = Dot_Colors_1[Random_Theme_1], ecolor = Error_Bar_colors_1[Random_Theme_1] , capsize=15)\n",
        "    graph_name = \"Relative Intensity by Mass in Parent Molecule Fragmentation\" + \" \" + self.MSDI.date_data_was_taken\n",
        "    for i, txt in enumerate(self.error.Parent_ratios):\n",
        "        plt.annotate(txt, (x_axis[i], y_axis[i]))\n",
        "    if PDF_Maker is not None:\n",
        "      PDF_Maker.pdf_pages.savefig(dpi=300)\n",
        "      plt.close()\n",
        "    else:\n",
        "      plt.show()\n",
        "\n",
        "    plt.figure(figsize = (10,5))\n",
        "    Column_labels = ['Mass', 'Relative Intensity']\n",
        "    x_axis = self.error.masses_D # rounded\n",
        "    y_axis = self.error.daughter_total_areas.tolist()\n",
        "    yerr = self.error.Error_D\n",
        "    plt.plot(x_axis, y_axis, color = '#1a2b6d') if self.professional else plt.plot(x_axis, y_axis, color = Line_Colors_2[Random_Theme_2])\n",
        "    plt.errorbar(x_axis, y_axis, yerr=yerr, fmt='s', color ='#8b0000', ecolor= '#888888', capsize=15) if self.professional else plt.errorbar(x_axis, y_axis, yerr=yerr, fmt='s', color = Dot_Colors_2[Random_Theme_2], ecolor = Error_Bar_colors_2[Random_Theme_2], capsize=15)\n",
        "    plt.xlabel('Mass')\n",
        "    plt.ylabel('Relative intensity')\n",
        "    plt.title(\"Relative Intensity by Mass in Daughter Molecule Fragmentation\" + \" \" + self.MSDI.date_data_was_taken)\n",
        "    graph_name = \"Relative Intensity by Mass in Daughter Molecule Fragmentation\" + \" \" + self.MSDI.date_data_was_taken\n",
        "    for i, txt in enumerate(self.error.Daughter_ratios):\n",
        "        plt.annotate(txt, (x_axis[i], y_axis[i]))\n",
        "    if PDF_Maker is not None:\n",
        "      PDF_Maker.pdf_pages.savefig(dpi=300)\n",
        "      plt.close()\n",
        "    else:\n",
        "      plt.show()\n",
        "\n",
        "class MakePDF:\n",
        "    \"\"\" Example Use of MakePDF:\n",
        "\n",
        "    Use the MakePDF class with a 'with' statement:\n",
        "    with MakePDF(\"example_with_context.pdf\", \"path/to/save\") as pdf_creator:\n",
        "    pdf_creator.add_chart(generate_line_chart, data)\n",
        "    Add a table to the PDF:\n",
        "    pdf_creator.add_table(table_data, column_names=column_names, additional_rows=additional_rows)\n",
        "\n",
        "    Alternatively, you can use each function individually if the table requires some changes that you don't want to manually type out\n",
        "    \"\"\"\n",
        "    def __init__(self, pdf_filename, file_path=None):\n",
        "        \"\"\"\n",
        "        Initializes the PDF generator.\n",
        "\n",
        "        Args:\n",
        "            pdf_filename (str): The name of the PDF file where content will be saved.\n",
        "        \"\"\"\n",
        "        # Handle file path; use pdf_filename if no path provided\n",
        "        if not pdf_filename.endswith('.pdf'):\n",
        "            pdf_filename += '.pdf'\n",
        "        self.pdf_path = pdf_filename if not file_path else file_path + '/' + pdf_filename\n",
        "        self.pdf_pages = PdfPages(self.pdf_path)\n",
        "        self.table_data = None # Holds the table itself in it's list of list form.\n",
        "        self.debug = False # Set to True if you want to have debug statements on.\n",
        "        self.warnings = False\n",
        "        self.Errors = False\n",
        "        self.notice = False\n",
        "\n",
        "    def pivot_table_data(self, *args):\n",
        "        \"\"\"\n",
        "        Pivots columns of lists of data into rows for the table.\n",
        "\n",
        "        Args:\n",
        "            *args: Multiple lists of data to be arranged vertically into table rows.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of rows (lists), each containing values from the input lists.\n",
        "        \"\"\"\n",
        "        # Combine the data into rows (transpose)\n",
        "        table_data = list(zip(*args))  # This effectively transposes the data\n",
        "        return table_data\n",
        "\n",
        "    def handle_floats_and_transpose(self, data_sets, Number_of_decimals = None):\n",
        "      float_counter = 1\n",
        "      placeholder_map = {}  # to store float->placeholder mappings\n",
        "      transformed_data_sets = []\n",
        "\n",
        "      # Step 1: Replace float values with unique codes\n",
        "      for data in data_sets:\n",
        "          print(f\"Processing data set: {data}\") if self.debug else None # Check each data set before processing\n",
        "          transformed_data = []\n",
        "\n",
        "          # Ensure all rows have the same length\n",
        "          row_lengths = [len(row) for row in data]\n",
        "          if len(set(row_lengths)) > 1:\n",
        "              print(f\"Warning: Rows have unequal lengths in data set. Skipping transposition for inconsistent data.\") if self.warnings else None\n",
        "              continue  # Or handle the rows in some other way\n",
        "\n",
        "          for row in data:\n",
        "              print(f\"Processing row: {row}\") if self.debug else None  # Check each row\n",
        "              transformed_row = []\n",
        "              for value in row:\n",
        "                  print(f\"Processing value: {value} (type: {type(value)})\") if self.debug else None # Check the value\n",
        "                  if isinstance(value, np.float64):\n",
        "                      # Assign a unique code to the floating point value\n",
        "                      placeholder = f'FLOAT{float_counter}'\n",
        "                      transformed_row.append(placeholder)\n",
        "                      placeholder_map[placeholder] = value  # map the placeholder to the actual value\n",
        "                      float_counter += 1\n",
        "                  else:\n",
        "                      transformed_row.append(value)\n",
        "              transformed_data.append(transformed_row)\n",
        "          transformed_data_sets.append(transformed_data)\n",
        "\n",
        "      # Step 2: Transpose the data (pivot)\n",
        "      print(f\"Transposing data sets...\") if self.debug else None\n",
        "      pivoted_data_sets = [list(zip(*data)) for data in transformed_data_sets]\n",
        "\n",
        "      # Step 3: Restore the floats from the placeholders\n",
        "      restored_data_sets = []\n",
        "      for data in pivoted_data_sets:\n",
        "          restored_data = []\n",
        "          for row in data:\n",
        "              restored_row = []\n",
        "              for value in row:\n",
        "                  if isinstance(value, str) and value.startswith('FLOAT'):\n",
        "                      # Replace the placeholder with the original value\n",
        "                      original_value = placeholder_map.get(value, value)\n",
        "                      if Number_of_decimals is not None:\n",
        "                        # Round to the specified number of decimals\n",
        "                        original_value = round(original_value, Number_of_decimals)\n",
        "                      restored_row.append(original_value)\n",
        "                      \"\"\"# Replace the placeholder with the original value\n",
        "                      restored_row.append(placeholder_map.get(value, value))\"\"\"\n",
        "                  else:\n",
        "                      restored_row.append(value)\n",
        "              restored_data.append(restored_row)\n",
        "          restored_data_sets.append(restored_data)\n",
        "\n",
        "      return restored_data_sets\n",
        "\n",
        "    def define_table(self, column_names, row_names, *data_sets, pivot=False, number_of_decimals = None):\n",
        "      \"\"\"\n",
        "      Defines the table with the specified row names, column headers, and multiple data sets.\n",
        "      Optionally pivots each data set (switches rows and columns).\n",
        "\n",
        "      Args:\n",
        "          column_names (list): List of column names (e.g., ['File 0', 'File 1', 'File 2']).\n",
        "          row_names (list): List of row names (e.g., ['Alpha', 'Enrichment', 'Error']).\n",
        "          *data_sets (lists): Multiple lists of data (e.g., [enrichment_data], [alpha_data], [ratio_data]).\n",
        "          pivot (bool): If True, pivots the data sets (rows become columns and columns become rows).\n",
        "          number_of_decimals (int): Number of decimal places to include when displaying data. It will round the last digit to the number of decimals specified.\n",
        "      Returns:\n",
        "          list: A 2D list representing the table with row names, column headers, and data.\n",
        "      \"\"\"\n",
        "      # Check if we need to pivot\n",
        "      # Step 1: Handle floaters and transpose the data\n",
        "      if pivot:\n",
        "            # Use handle_floats_and_transpose to replace floats, transpose, and restore values\n",
        "            data_sets = self.handle_floats_and_transpose(data_sets, Number_of_decimals = number_of_decimals)\n",
        "      else:\n",
        "        # If not pivoting, handle rounding directly if it is specified\n",
        "        if number_of_decimals is not None:\n",
        "            rounded_data_sets = []\n",
        "            for data in data_sets:\n",
        "                rounded_data = [\n",
        "                    [round(value, number_of_decimals) if isinstance(value, (float, np.float64)) else value for value in row]\n",
        "                    for row in data\n",
        "                ]\n",
        "                rounded_data_sets.append(rounded_data)\n",
        "            data_sets = rounded_data_sets\n",
        "\n",
        "      # Step 2: Create the final table with an empty cell in the first column and column headers\n",
        "      table_data = []\n",
        "\n",
        "      # Add the column headers to the first row (empty cell for 'File')\n",
        "      table_data.append([''] + row_names.tolist())\n",
        "\n",
        "      # Add each row (file) from the pivoted data\n",
        "      for i, file_name in enumerate(column_names):\n",
        "            row = [file_name]  # Add the file name in the first column\n",
        "            for data in data_sets:\n",
        "                row.extend(data[i])  # Add data corresponding to each file\n",
        "            table_data.append(row)\n",
        "\n",
        "      self.table_data = table_data\n",
        "      return table_data\n",
        "\n",
        "    def truncate_text(cell_text, max_char):\n",
        "        \"\"\"Truncates text to fit within a specified character limit per cell.\"\"\"\n",
        "        if isinstance(cell_text, str) and len(cell_text) > max_char:\n",
        "            return cell_text[:max_char] + \"...\"  # Truncate and add ellipsis\n",
        "        return cell_text  # Return the text if it's short enough\n",
        "\n",
        "    def add_table(self, table_data, title = 'I owe you a title', Additional_Information = None):\n",
        "        \"\"\"\n",
        "        Adds a table to the PDF.\n",
        "        Args:\n",
        "            table_data (list): A 2D list representing the table data. You can pass in define_table() here\n",
        "            title (str): A title for the table.\n",
        "            Additional_Information (str): Additional information to be displayed below the table. Such as Isolations and NCE values.\n",
        "        \"\"\"\n",
        "        # Create table plot\n",
        "        ax = plt.gca()\n",
        "        ax.axis('tight')\n",
        "        ax.axis('off')\n",
        "\n",
        "        # Create the table\n",
        "        table = plt.table(\n",
        "            cellText=table_data,\n",
        "            loc='center',\n",
        "            colWidths=[0.2] * (len(table_data[0]) if table_data else 1),\n",
        "            cellLoc='center'\n",
        "        )\n",
        "\n",
        "        # Adjust table appearance\n",
        "        table.auto_set_font_size(False)\n",
        "        table.set_fontsize(12)  # Set a readable font size\n",
        "        table.scale(1.5, 1.5)   # Scale table width and height for better visibility\n",
        "\n",
        "        # Add a title if provided\n",
        "        if title:\n",
        "            plt.title(title, fontsize=14, weight='bold', pad=20)  # Add spacing from the table\n",
        "\n",
        "        table.auto_set_font_size(False)\n",
        "        table.set_fontsize(10)  # Adjust font size to fit content better\n",
        "\n",
        "        if Additional_Information is not None:\n",
        "          # Add additional information below the table\n",
        "          ax.text(0.5, -0.5, Additional_Information,\n",
        "          fontsize=10, ha='center', transform=ax.transAxes)\n",
        "\n",
        "        #Alternative method to fit data in the cell if it is preferable\n",
        "        \"\"\" #Apply truncation to all cells in the table data\n",
        "        truncated_table_data = [\n",
        "            [truncate_text(str(cell), max_char) for cell in row]\n",
        "            for row in table_data\n",
        "        ]\"\"\"\n",
        "\n",
        "        # Save the table to the PDF\n",
        "        self.pdf_pages.savefig(bbox_inches='tight', dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "    def add_chart(self, chart_function, **additional_arguments):\n",
        "        \"\"\"\n",
        "        Adds a chart to the PDF using an existing chart function.\n",
        "\n",
        "        Args:\n",
        "            chart_function (function): A pre-existing function that generates a chart (e.g., `overlaid_line_plot_AVI,`).\n",
        "            **additional_arguments: Additional arguments to pass to the chart function.\n",
        "        \"\"\"\n",
        "        # Generate the chart using the provided chart function\n",
        "        plt.figure(figsize = (10,5))\n",
        "        chart_function()\n",
        "        # Save the chart to the PDF\n",
        "        chart_function(**additional_arguments)  # Pass kwargs to the chart function\n",
        "        self.pdf_pages.savefig(dpi=300)\n",
        "        plt.close()\n",
        "\n",
        "    def save_pdf(self):\n",
        "        \"\"\"Closes the PDF file and saves all pages.\"\"\"\n",
        "        self.pdf_pages.close()\n",
        "\n",
        "    # If you want to use a with statement, these functions are implemented for this reason\n",
        "    def __enter__(self):\n",
        "        \"\"\"Start the context and open the PDF for writing.\"\"\"\n",
        "        self.pdf_pages = PdfPages(self.pdf_path)\n",
        "        return self  # Return the instance for use within the 'with' block\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        \"\"\"End the context and close the PDF.\"\"\"\n",
        "        if self.pdf_pages:\n",
        "            self.pdf_pages.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "lEm8r0hQv4OQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}